#!/usr/bin/env python3
"""
æ”¹è¿›çš„æ¨ç†è„šæœ¬ - æ”¯æŒä¸‰åˆ†æ”¯ç‹¬ç«‹åŠ è½½
ä»ç‹¬ç«‹çš„root/trans/poseæ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹
"""

import os
import random
import copy
from datetime import datetime
from pathlib import Path
from diffusers.utils import logging
from typing import Optional, List, Union, Dict, Tuple
import yaml
import pickle

import imageio
import json
import numpy as np
import torch
import torch.nn as nn
from safetensors import safe_open
from safetensors.torch import load_file
from PIL import Image
import torchvision.transforms.functional as TVF
from transformers import (
    T5EncoderModel,
    T5Tokenizer,
    AutoModelForCausalLM,
    AutoProcessor,
    AutoTokenizer,
    AutoConfig,
)
from huggingface_hub import hf_hub_download
from dataclasses import dataclass, field

from ltx_video.models.autoencoders.causal_video_autoencoder import (
    CausalVideoAutoencoder,
    Decoder
)
from ltx_video.models.autoencoders.motion_decoder import (
    ElasticMotionDecoder,
    ElasticMotionOnlyDecoder,
    create_elastic_motion_decoder_config
)
from ltx_video.models.transformers.symmetric_patchifier import SymmetricPatchifier
from ltx_video.models.transformers.transformer3d import Transformer3DModel
from ltx_video.pipelines.pipeline_ltx_video import (
    ConditioningItem,
    LTXVideoPipeline,
    LTXMultiScalePipeline,
    vae_decode_motion,
    MotionVAEOutput,
    save_motion_params,
)
from ltx_video.schedulers.rf import RectifiedFlowScheduler
from ltx_video.utils.skip_layer_strategy import SkipLayerStrategy
from ltx_video.models.autoencoders.latent_upsampler import LatentUpsampler
import ltx_video.pipelines.crf_compressor as crf_compressor

logger = logging.get_logger("LTX-Video")

# è®¾ç½®ç¦»çº¿æ¨¡å¼
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ['DIFFUSERS_OFFLINE'] = '1'


def get_total_gpu_memory():
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        return total_memory
    return 0


def get_device():
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    return "cpu"


class DummyEncoder(nn.Module):
    """è™šæ‹Ÿçš„encoderï¼Œç”¨äºå…¼å®¹éœ€è¦è®¿é—®encoderå±æ€§çš„ä»£ç """

    def __init__(self, out_channels=128):
        super().__init__()
        self.down_blocks = []
        self.out_channels = out_channels
        self.config = {
            "out_channels": out_channels,
            "_class_name": "DummyEncoder",
            "norm_num_groups": 32,
            "double_z": False,
            "sample_size": 64,
        }

    def __len__(self):
        return 0

    def __getattr__(self, name):
        if name == 'down_blocks':
            return self.down_blocks
        elif name == 'out_channels':
            return self.out_channels
        elif name == 'config':
            return self.config
        raise AttributeError(f"'DummyEncoder' object has no attribute '{name}'")

    def forward(self, x):
        raise NotImplementedError("DummyEncoderåªç”¨äºå ä½ï¼Œä¸æ”¯æŒå‰å‘ä¼ æ’­")

    def to(self, *args, **kwargs):
        return self

    def eval(self):
        return self

    def train(self, mode=True):
        return self


class TriBranchMotionDecoderOnly(nn.Module):
    """
    ä¸‰åˆ†æ”¯ç‹¬ç«‹çš„è¿åŠ¨è§£ç å™¨å°è£…
    åˆ†åˆ«åŠ è½½root/trans/poseä¸‰ä¸ªåˆ†æ”¯çš„æ£€æŸ¥ç‚¹
    """

    def __init__(
            self,
            root_checkpoint_path: str,
            trans_checkpoint_path: str,
            pose_checkpoint_path: str,
            device: str = "cuda",
            latent_channels: int = 128,
            motion_channels_per_person: int = 69,
            temporal_downscale_factor: int = 8,
            spatial_downscale_factor: int = 1,
            causal: bool = True,
            timestep_conditioning: bool = True,
    ):
        super().__init__()

        self.is_motion_vae = True
        self.motion_channels_per_person = motion_channels_per_person
        self.latent_channels = latent_channels
        self.temporal_downscale_factor = temporal_downscale_factor
        self.spatial_downscale_factor = spatial_downscale_factor
        self.scaling_factor = nn.Parameter(torch.tensor(1.0))
        self.dtype = torch.float32  # åˆå§‹è®¾ä¸ºfloat32
        self.device = device

        print("=" * 60)
        print("ğŸ¯ åŠ è½½ä¸‰åˆ†æ”¯ç‹¬ç«‹è®­ç»ƒçš„è¿åŠ¨è§£ç å™¨")
        print("=" * 60)

        # åŠ è½½ä¸‰ä¸ªåˆ†æ”¯çš„è§£ç å™¨
        self.root_decoder = self._load_single_branch_decoder(
            checkpoint_path=root_checkpoint_path,
            branch_name="root",
            motion_channels=3,  # rootåˆ†æ”¯è¾“å‡º3ç»´
            device=device
        )

        self.trans_decoder = self._load_single_branch_decoder(
            checkpoint_path=trans_checkpoint_path,
            branch_name="trans",
            motion_channels=3,  # transåˆ†æ”¯è¾“å‡º3ç»´
            device=device
        )

        self.pose_decoder = self._load_single_branch_decoder(
            checkpoint_path=pose_checkpoint_path,
            branch_name="pose",
            motion_channels=63,  # poseåˆ†æ”¯è¾“å‡º63ç»´
            device=device
        )

        # è™šæ‹Ÿencoderç”¨äºå…¼å®¹
        self.encoder = DummyEncoder(out_channels=latent_channels)

        print("=" * 60)
        print("âœ… ä¸‰åˆ†æ”¯æ¨¡å‹åŠ è½½å®Œæˆ")
        print("=" * 60)

        # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è½¬æ¢ä¸ºfloat16ä»¥åŒ¹é…ç®¡é“
        self.to(device)
        self.eval()
        
        # è½¬æ¢ä¸ºfloat16
        self._convert_to_float16()

    def _convert_to_float16(self):
        """å°†è§£ç å™¨è½¬æ¢ä¸ºfloat16ä»¥åŒ¹é…ç®¡é“ç²¾åº¦"""
        print("ğŸ”§ å°†è§£ç å™¨è½¬æ¢ä¸ºfloat16ä»¥åŒ¹é…ç®¡é“ç²¾åº¦...")
        
        def convert_module(module):
            for param in module.parameters():
                if param.dtype == torch.float32:
                    param.data = param.data.to(torch.float16)
            for buffer in module.buffers():
                if buffer.dtype == torch.float32:
                    buffer.data = buffer.data.to(torch.float16)
        
        convert_module(self.root_decoder)
        convert_module(self.trans_decoder)
        convert_module(self.pose_decoder)
        
        # æ›´æ–°dtype
        self.dtype = torch.float16
        print("âœ… è§£ç å™¨å·²è½¬æ¢ä¸ºfloat16")

    def _load_single_branch_decoder(
            self,
            checkpoint_path: str,
            branch_name: str,
            motion_channels: int,
            device: str
    ) -> ElasticMotionOnlyDecoder:
        """åŠ è½½å•ä¸ªåˆ†æ”¯çš„è§£ç å™¨"""
        print(f"\nğŸ“¥ åŠ è½½{branch_name}åˆ†æ”¯æ£€æŸ¥ç‚¹: {checkpoint_path}")

        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"{branch_name}åˆ†æ”¯æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")

        try:
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
            print(f"  âœ“ {branch_name}æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")

            # ğŸ”¥ æ ¹æ®è®­ç»ƒä»£ç ï¼Œæƒé‡åœ¨'decoder_state_dict'ä¸­
            # ä½†ç»è¿‡æå–å·¥å…·å¤„ç†åï¼Œæƒé‡åœ¨'branch_state_dict'ä¸­
            checkpoint_keys = list(checkpoint.keys())
            print(f"  ğŸ“‹ æ£€æŸ¥ç‚¹é”®æ•°é‡: {len(checkpoint_keys)}")
            print(f"  ğŸ“‹ æ£€æŸ¥ç‚¹é”®: {checkpoint_keys}")

            # ä¼˜å…ˆä½¿ç”¨æå–åçš„åˆ†æ”¯æƒé‡
            state_dict = None
            if 'branch_state_dict' in checkpoint:
                state_dict = checkpoint['branch_state_dict']
                print(f"  âœ… ä»'branch_state_dict'åŠ è½½æƒé‡")
            elif 'decoder_state_dict' in checkpoint:
                state_dict = checkpoint['decoder_state_dict']
                print(f"  âœ… ä»'decoder_state_dict'åŠ è½½æƒé‡")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†é”®ï¼Œæ£€æŸ¥ç‚¹å¯èƒ½å°±æ˜¯state_dictæœ¬èº«
                print(f"  ğŸ” æ£€æŸ¥ç‚¹å¯èƒ½å°±æ˜¯state_dictæœ¬èº«")
                state_dict = checkpoint

            # ä»æ£€æŸ¥ç‚¹è·å–æ·±åº¦ä¿¡æ¯
            current_depth = checkpoint.get('current_depth', 2)
            max_depth = checkpoint.get('max_depth', 20)

            print(f"  ğŸ“Š {branch_name}é…ç½®: å½“å‰æ·±åº¦={current_depth}, æœ€å¤§æ·±åº¦={max_depth}")

            # åˆ›å»ºå•ä¸ªåˆ†æ”¯çš„è§£ç å™¨
            decoder = ElasticMotionOnlyDecoder(
                dims=3,
                in_channels=self.latent_channels,
                motion_channels_per_person=motion_channels,
                base_channels=128,
                norm_layer="group_norm",
                causal=self.causal,
                timestep_conditioning=self.timestep_conditioning,
                spatial_padding_mode="zeros",
                dropout_rate=0.1,
                use_layer_norm=False,
                use_stochastic_depth=True,
                stochastic_depth_rate=0.1,
                max_res_layers=max_depth,
                initial_res_layers=current_depth,
                use_elastic_depth=True,
            )

            # åŠ è½½æƒé‡
            print(f"  ğŸ”§ åŠ è½½æƒé‡åˆ°{branch_name}è§£ç å™¨...")
            decoder.load_state_dict(state_dict, strict=True)
            print(f"  âœ… {branch_name}åˆ†æ”¯æƒé‡åŠ è½½æˆåŠŸ")

            # ç»Ÿè®¡å‚æ•°
            total_params = sum(p.numel() for p in decoder.parameters())
            print(f"  ğŸ“Š {branch_name}åˆ†æ”¯å‚æ•°é‡: {total_params:,}")

            decoder.to(device)
            decoder.eval()

            return decoder

        except Exception as e:
            print(f"  âŒ {branch_name}åˆ†æ”¯åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    @property
    def causal(self):
        """è·å–å› æœæ€§é…ç½®"""
        return True  # æ ¹æ®ä½ çš„è®­ç»ƒå‚æ•°

    @property
    def timestep_conditioning(self):
        """è·å–æ—¶é—´æ­¥æ¡ä»¶é…ç½®"""
        return True  # æ ¹æ®ä½ çš„è®­ç»ƒå‚æ•°

    def decode(self, latents, target_shape, timestep=None, return_dict=True):
        """
        ä¸‰åˆ†æ”¯ç‹¬ç«‹è§£ç 
        è¾“å…¥: latents [batch, latent_channels, T_compressed, 1, n_persons]
        è¾“å‡º: motion [batch, 69, target_frames, 1, n_persons]
        """
        batch_size, channels, T_compressed, H, W = latents.shape
        n_persons = W
        target_frames = target_shape[2]

        # éªŒè¯è¾“å…¥
        assert H == 1, f"é«˜åº¦ç»´åº¦åº”ä¸º1ï¼Œå½“å‰ä¸º{H}"
        assert n_persons > 0, f"å®½åº¦ç»´åº¦ï¼ˆäººæ•°ï¼‰åº”å¤§äº0ï¼Œå½“å‰ä¸º{W}"
        assert channels == self.latent_channels, \
            f"è¾“å…¥é€šé“æ•°{channels} != é¢„æœŸé€šé“æ•°{self.latent_channels}"

        # ğŸ”¥ ç¡®ä¿latentsä¸è§£ç å™¨åœ¨åŒä¸€æ•°æ®ç±»å‹ä¸Š
        if latents.dtype != self.dtype:
            print(f"âš ï¸  è½¬æ¢latentsç±»å‹: {latents.dtype} -> {self.dtype}")
            latents = latents.to(self.dtype)

        # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†timestepå‚æ•°
        if timestep is not None:
            # å¦‚æœtimestepæ˜¯æµ®ç‚¹æ•°æˆ–æ•´æ•°ï¼Œè½¬æ¢ä¸ºå¼ é‡
            if isinstance(timestep, (int, float)):
                print(f"ğŸ”§ è½¬æ¢timestepç±»å‹: {type(timestep)} -> tensor")
                timestep = torch.tensor([timestep], device=latents.device, dtype=latents.dtype)
            # ç¡®ä¿timestepæœ‰æ­£ç¡®çš„å½¢çŠ¶ [batch_size]
            if isinstance(timestep, torch.Tensor):
                if timestep.dim() == 0:
                    timestep = timestep.unsqueeze(0)
                # ç¡®ä¿timestepå¹¿æ’­åˆ°batch_size
                if timestep.shape[0] != batch_size:
                    if timestep.shape[0] == 1:
                        timestep = timestep.expand(batch_size)
                    else:
                        raise ValueError(f"timestepå½¢çŠ¶{timestep.shape}ä¸batch_size{batch_size}ä¸åŒ¹é…")
            
            print(f"âœ… timestepå½¢çŠ¶: {timestep.shape}, dtype: {timestep.dtype}")
        else:
            # å¦‚æœtimestepæ˜¯Noneï¼Œåˆ›å»ºä¸€ä¸ªéšæœºtimestepï¼ˆè®­ç»ƒæ—¶å°±æ˜¯è¿™æ ·çš„ï¼‰
            print(f"âš ï¸  timestepä¸ºNoneï¼Œåˆ›å»ºéšæœºtimestep")
            timestep = torch.rand(batch_size, device=latents.device, dtype=latents.dtype)
            print(f"âœ… åˆ›å»ºéšæœºtimestep: {timestep.shape}, dtype: {timestep.dtype}")

        # ä¸‰ä¸ªåˆ†æ”¯åˆ†åˆ«è§£ç 
        target_shape_branch = (batch_size, 3, target_frames, 1, n_persons)

        # transè§£ç 
        trans_output = self.trans_decoder(
            latents,
            target_shape=target_shape_branch,
            timestep=timestep
        )

        # rootè§£ç 
        root_output = self.root_decoder(
            latents,
            target_shape=target_shape_branch,
            timestep=timestep
        )

        # poseè§£ç 
        pose_target_shape = (batch_size, 63, target_frames, 1, n_persons)
        pose_output = self.pose_decoder(
            latents,
            target_shape=pose_target_shape,
            timestep=timestep
        )

        # æ‹¼æ¥ä¸‰ä¸ªéƒ¨åˆ†
        motion = torch.cat([trans_output, root_output, pose_output], dim=1)

        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        target_shape_full = (batch_size, 69, target_frames, 1, n_persons)
        assert motion.shape == target_shape_full, \
            f"è¾“å‡ºå½¢çŠ¶{motion.shape} != ç›®æ ‡å½¢çŠ¶{target_shape_full}"

        if return_dict:
            return {
                "motion_params": motion,
                "latents": latents,
                "target_frames": target_frames,
                "num_persons": n_persons,
                "trans_output": trans_output,
                "root_output": root_output,
                "pose_output": pose_output,
                "timestep": timestep,
            }
        else:
            return motion

    def split_by_person(self, motion_output: torch.FloatTensor) -> List[torch.FloatTensor]:
        """
        å°†è¿åŠ¨è¾“å‡ºæŒ‰äººåˆ†å‰²
        """
        from einops import rearrange

        if isinstance(motion_output, dict):
            motion_params = motion_output["motion_params"]
        else:
            motion_params = motion_output

        batch_size, channels, T, H, n_persons = motion_params.shape

        persons_motion = []
        for i in range(n_persons):
            person_motion = motion_params[:, :, :, :, i:i + 1]
            person_motion = rearrange(person_motion, 'b c t 1 1 -> b t c')
            persons_motion.append(person_motion)

        return persons_motion

    def to(self, *args, **kwargs):
        """ç§»åŠ¨åˆ°è®¾å¤‡"""
        self.root_decoder = self.root_decoder.to(*args, **kwargs)
        self.trans_decoder = self.trans_decoder.to(*args, **kwargs)
        self.pose_decoder = self.pose_decoder.to(*args, **kwargs)
        return self

    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.root_decoder.eval()
        self.trans_decoder.eval()
        self.pose_decoder.eval()
        return self

    def train(self, mode=True):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.root_decoder.train(mode)
        self.trans_decoder.train(mode)
        self.pose_decoder.train(mode)
        return self


def load_trained_motion_decoder(
        root_checkpoint_path: str,
        trans_checkpoint_path: str,
        pose_checkpoint_path: str,
        device: str = "cuda",
        latent_channels: int = 128,
        motion_channels_per_person: int = 69,
):
    """
    åŠ è½½ä¸‰åˆ†æ”¯ç‹¬ç«‹è®­ç»ƒçš„è¿åŠ¨è§£ç å™¨
    """
    print("=" * 60)
    print("ğŸ“¥ åŠ è½½ä¸‰åˆ†æ”¯ç‹¬ç«‹è®­ç»ƒçš„è¿åŠ¨è§£ç å™¨")
    print("=" * 60)

    # è§£æè·¯å¾„
    def resolve_path(path):
        if not os.path.isabs(path):
            current_dir = Path(__file__).parent
            return (current_dir / path).resolve()
        return Path(path)

    root_path = resolve_path(root_checkpoint_path)
    trans_path = resolve_path(trans_checkpoint_path)
    pose_path = resolve_path(pose_checkpoint_path)

    print(f"  rootåˆ†æ”¯è·¯å¾„: {root_path}")
    print(f"  transåˆ†æ”¯è·¯å¾„: {trans_path}")
    print(f"  poseåˆ†æ”¯è·¯å¾„: {pose_path}")

    # åˆ›å»ºä¸‰åˆ†æ”¯è§£ç å™¨
    motion_vae = TriBranchMotionDecoderOnly(
        root_checkpoint_path=str(root_path),
        trans_checkpoint_path=str(trans_path),
        pose_checkpoint_path=str(pose_path),
        device=device,
        latent_channels=latent_channels,
        motion_channels_per_person=motion_channels_per_person,
        temporal_downscale_factor=8,
        spatial_downscale_factor=1,
        causal=True,  # ä½ çš„è®­ç»ƒå‚æ•° --causal
        timestep_conditioning=True  # ä½ çš„è®­ç»ƒå‚æ•° --use_timestep
    )

    print(f"âœ… ä¸‰åˆ†æ”¯MotionDecoderåŠ è½½æˆåŠŸ")
    print(f"  è®¾å¤‡: {device}")

    # ç»Ÿè®¡å‚æ•°
    root_params = sum(p.numel() for p in motion_vae.root_decoder.parameters())
    trans_params = sum(p.numel() for p in motion_vae.trans_decoder.parameters())
    pose_params = sum(p.numel() for p in motion_vae.pose_decoder.parameters())
    total_params = root_params + trans_params + pose_params

    print(f"  å‚æ•°ç»Ÿè®¡:")
    print(f"    rootåˆ†æ”¯: {root_params:,}")
    print(f"    transåˆ†æ”¯: {trans_params:,}")
    print(f"    poseåˆ†æ”¯: {pose_params:,}")
    print(f"    æ€»å‚æ•°: {total_params:,}")

    return motion_vae


def load_image_to_tensor_with_resize_and_crop(
        image_input: Union[str, Image.Image],
        target_height: int = 512,
        target_width: int = 768,
        just_crop: bool = False,
) -> torch.Tensor:
    if isinstance(image_input, str):
        image = Image.open(image_input).convert("RGB")
    elif isinstance(image_input, Image.Image):
        image = image_input
    else:
        raise ValueError("image_input must be either a file path or a PIL Image object")

    input_width, input_height = image.size
    aspect_ratio_target = target_width / target_height
    aspect_ratio_frame = input_width / input_height
    if aspect_ratio_frame > aspect_ratio_target:
        new_width = int(input_height * aspect_ratio_target)
        new_height = input_height
        x_start = (input_width - new_width) // 2
        y_start = 0
    else:
        new_width = input_width
        new_height = int(input_width / aspect_ratio_target)
        x_start = 0
        y_start = (input_height - new_height) // 2

    image = image.crop((x_start, y_start, x_start + new_width, y_start + new_height))
    if not just_crop:
        image = image.resize((target_width, target_height))

    frame_tensor = TVF.to_tensor(image)
    frame_tensor = TVF.gaussian_blur(frame_tensor, kernel_size=3, sigma=1.0)
    frame_tensor_hwc = frame_tensor.permute(1, 2, 0)
    frame_tensor_hwc = crf_compressor.compress(frame_tensor_hwc)
    frame_tensor = frame_tensor_hwc.permute(2, 0, 1) * 255.0
    frame_tensor = (frame_tensor / 127.5) - 1.0
    return frame_tensor.unsqueeze(0).unsqueeze(2)


def calculate_padding(
        source_height: int, source_width: int, target_height: int, target_width: int
) -> tuple[int, int, int, int]:
    pad_height = target_height - source_height
    pad_width = target_width - source_width

    pad_top = pad_height // 2
    pad_bottom = pad_height - pad_top
    pad_left = pad_width // 2
    pad_right = pad_width - pad_left

    padding = (pad_left, pad_right, pad_top, pad_bottom)
    return padding


def convert_prompt_to_filename(text: str, max_len: int = 20) -> str:
    clean_text = "".join(
        char.lower() for char in text if char.isalpha() or char.isspace()
    )

    words = clean_text.split()

    result = []
    current_length = 0

    for word in words:
        new_length = current_length + len(word)

        if new_length <= max_len:
            result.append(word)
            current_length += len(word)
        else:
            break

    return "-".join(result)


def get_unique_filename(
        base: str,
        ext: str,
        prompt: str,
        seed: int,
        resolution: tuple[int, int, int],
        dir: Path,
        endswith=None,
        index_range=1000,
) -> Path:
    base_filename = f"{base}_{convert_prompt_to_filename(prompt, max_len=30)}_{seed}_{resolution[0]}x{resolution[1]}x{resolution[2]}"
    for i in range(index_range):
        filename = dir / f"{base_filename}_{i}{endswith if endswith else ''}{ext}"
        if not os.path.exists(filename):
            return filename
    raise FileExistsError(
        f"Could not find a unique filename after {index_range} attempts."
    )


def seed_everething(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    if torch.backends.mps.is_available():
        torch.mps.manual_seed(seed)


def check_local_path(path: str, model_name: str) -> str:
    if not path:
        raise FileNotFoundError(f"{model_name} è·¯å¾„æœªé…ç½®")

    if not os.path.isabs(path):
        current_dir = Path(__file__).parent
        abs_path = (current_dir / path).resolve()
    else:
        abs_path = Path(path)

    if not os.path.exists(abs_path):
        raise FileNotFoundError(f"{model_name} æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {abs_path}")

    if not (os.path.isfile(abs_path) or os.path.isdir(abs_path)):
        raise FileNotFoundError(f"{model_name} è·¯å¾„æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {abs_path}")

    print(f"âœ“ {model_name} æ‰¾åˆ°æœ¬åœ°æ–‡ä»¶: {abs_path}")
    return str(abs_path)


def check_model_directory_structure(path: str, required_files: list = None):
    path = Path(path)

    if not path.exists():
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {path}")

    if path.is_file():
        print(f"æ¨¡å‹è·¯å¾„æ˜¯æ–‡ä»¶: {path}")
        return

    print(f"æ£€æŸ¥æ¨¡å‹ç›®å½•ç»“æ„: {path}")

    common_files = ["config.json", "pytorch_model.bin", "model.safetensors", "tokenizer.json", "tokenizer_config.json"]
    if required_files:
        common_files.extend(required_files)

    found_files = []
    for file in common_files:
        file_path = path / file
        if file_path.exists():
            found_files.append(file)
            print(f"  âœ“ æ‰¾åˆ°: {file}")
        else:
            print(f"  âœ— æœªæ‰¾åˆ°: {file}")

    if not found_files:
        print(f"è­¦å‘Š: åœ¨ç›®å½• {path} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")

    subdirs = [d for d in path.iterdir() if d.is_dir()]
    for subdir in subdirs:
        print(f"  ğŸ“ å­ç›®å½•: {subdir.name}")


def create_transformer(ckpt_path: str, precision: str) -> Transformer3DModel:
    ckpt_path = check_local_path(ckpt_path, "Transformerä¸»æ¨¡å‹")

    if precision == "float8_e4m3fn":
        try:
            from q8_kernels.integration.patch_transformer import (
                patch_diffusers_transformer as patch_transformer_for_q8_kernels,
            )

            transformer = Transformer3DModel.from_pretrained(
                ckpt_path, local_files_only=True, dtype=torch.float8_e4m3fn
            )
            patch_transformer_for_q8_kernels(transformer)
            return transformer
        except ImportError:
            raise ValueError(
                "Q8-Kernels not found. To use FP8 checkpoint, please install Q8 kernels from https://github.com/Lightricks/LTXVideo-Q8-Kernels"
            )
    elif precision == "bfloat16":
        return Transformer3DModel.from_pretrained(ckpt_path, local_files_only=True).to(torch.bfloat16)
    else:
        return Transformer3DModel.from_pretrained(ckpt_path, local_files_only=True)


def load_transformers_model_with_fallback(model_path: str, model_class, **kwargs):
    model_path = check_local_path(model_path, f"{model_class.__name__}æ¨¡å‹")

    try:
        print(f"å°è¯•æ ‡å‡†æ–¹å¼åŠ è½½æ¨¡å‹: {model_path}")
        model = model_class.from_pretrained(model_path, local_files_only=True, **kwargs)
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        return model
    except Exception as e:
        print(f"æ ‡å‡†æ–¹å¼åŠ è½½å¤±è´¥: {e}")
        print(f"å°è¯•å¤‡é€‰æ–¹å¼åŠ è½½æ¨¡å‹...")

        try:
            model = model_class.from_pretrained(model_path, **kwargs)
            print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (å¤‡é€‰æ–¹å¼): {model_path}")
            return model
        except Exception as e2:
            print(f"å¤‡é€‰æ–¹å¼åŠ è½½å¤±è´¥: {e2}")

            model_dir = Path(model_path)
            if model_dir.is_dir():
                try:
                    config_path = model_dir / "config.json"
                    if config_path.exists():
                        print(f"å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_path}")
                        config = AutoConfig.from_pretrained(str(model_dir), local_files_only=True)

                        model_files = list(model_dir.glob("*.safetensors")) + list(model_dir.glob("*.bin"))
                        if model_files:
                            print(f"æ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶: {model_files[0]}")
                            model = model_class.from_pretrained(
                                str(model_dir),
                                config=config,
                                local_files_only=True,
                                **kwargs
                            )
                            print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (æ‰‹åŠ¨æ–¹å¼): {model_path}")
                            return model
                except Exception as e3:
                    print(f"æ‰‹åŠ¨æ–¹å¼åŠ è½½å¤±è´¥: {e3}")

            raise FileNotFoundError(f"æ— æ³•åŠ è½½æ¨¡å‹ {model_path}: {e2}")


def load_processor_with_fallback(processor_path: str, processor_class, **kwargs):
    processor_path = check_local_path(processor_path, f"{processor_class.__name__}å¤„ç†å™¨")

    try:
        print(f"å°è¯•åŠ è½½å¤„ç†å™¨: {processor_path}")
        processor = processor_class.from_pretrained(processor_path, local_files_only=True, **kwargs)
        print(f"âœ“ å¤„ç†å™¨åŠ è½½æˆåŠŸ: {processor_path}")
        return processor
    except Exception as e:
        print(f"å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
        print(f"å°è¯•å¤‡é€‰æ–¹å¼åŠ è½½å¤„ç†å™¨...")

        try:
            processor = processor_class.from_pretrained(processor_path, **kwargs)
            print(f"âœ“ å¤„ç†å™¨åŠ è½½æˆåŠŸ (å¤‡é€‰æ–¹å¼): {processor_path}")
            return processor
        except Exception as e2:
            print(f"å¤‡é€‰æ–¹å¼åŠ è½½å¤±è´¥: {e2}")

            processor_dir = Path(processor_path)
            if processor_dir.is_dir():
                try:
                    config_path = processor_dir / "processor_config.json"
                    if not config_path.exists():
                        config_path = processor_dir / "config.json"

                    if config_path.exists():
                        print(f"å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½å¤„ç†å™¨: {config_path}")
                        processor = processor_class.from_pretrained(str(processor_dir), **kwargs)
                        print(f"âœ“ å¤„ç†å™¨åŠ è½½æˆåŠŸ (æ‰‹åŠ¨æ–¹å¼): {processor_path}")
                        return processor
                except Exception as e3:
                    print(f"æ‰‹åŠ¨æ–¹å¼åŠ è½½å¤±è´¥: {e3}")

            raise FileNotFoundError(f"æ— æ³•åŠ è½½å¤„ç†å™¨ {processor_path}: {e2}")


def create_ltx_video_pipeline(
        ckpt_path: str,
        precision: str,
        text_encoder_model_name_or_path: str,
        sampler: Optional[str] = None,
        device: Optional[str] = None,
        enhance_prompt: bool = False,
        prompt_enhancer_image_caption_model_name_or_path: Optional[str] = None,
        prompt_enhancer_llm_model_name_or_path: Optional[str] = None,
        motion_mode: bool = False,
        motion_channels_per_person: int = 69,
        # ä¸‰åˆ†æ”¯æ£€æŸ¥ç‚¹è·¯å¾„
        root_checkpoint_path: str = None,
        trans_checkpoint_path: str = None,
        pose_checkpoint_path: str = None,
) -> LTXVideoPipeline:
    ckpt_path = check_local_path(ckpt_path, "ä¸»æ¨¡å‹checkpoint")
    text_encoder_model_name_or_path = check_local_path(text_encoder_model_name_or_path, "æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹")

    print("æ£€æŸ¥æ¨¡å‹ç›®å½•ç»“æ„:")
    check_model_directory_structure(text_encoder_model_name_or_path)

    # å®Œå…¨ç§»é™¤æç¤ºå¢å¼ºç›¸å…³çš„æ£€æŸ¥å’ŒåŠ è½½é€»è¾‘
    print("æç¤ºå¢å¼ºåŠŸèƒ½å·²å®Œå…¨å…³é—­")

    with safe_open(ckpt_path, framework="pt") as f:
        metadata = f.metadata()
        config_str = metadata.get("config")
        configs = json.loads(config_str)
        allowed_inference_steps = configs.get("allowed_inference_steps", None)

    # æ ¹æ®è¿åŠ¨æ¨¡å¼é€‰æ‹©VAEåˆ›å»ºæ–¹å¼
    if motion_mode:
        print("\n" + "=" * 50)
        print("ğŸ”„ è¿åŠ¨æ¨¡å¼ï¼šä½¿ç”¨ä¸‰åˆ†æ”¯ç‹¬ç«‹è®­ç»ƒçš„è¿åŠ¨è§£ç å™¨")
        print("=" * 50)

        # éªŒè¯ä¸‰ä¸ªæ£€æŸ¥ç‚¹è·¯å¾„
        if not all([root_checkpoint_path, trans_checkpoint_path, pose_checkpoint_path]):
            raise ValueError("è¿åŠ¨æ¨¡å¼éœ€è¦æä¾›rootã€transã€poseä¸‰ä¸ªåˆ†æ”¯çš„æ£€æŸ¥ç‚¹è·¯å¾„")

        print(f"  rootåˆ†æ”¯æ£€æŸ¥ç‚¹: {root_checkpoint_path}")
        print(f"  transåˆ†æ”¯æ£€æŸ¥ç‚¹: {trans_checkpoint_path}")
        print(f"  poseåˆ†æ”¯æ£€æŸ¥ç‚¹: {pose_checkpoint_path}")

        # ä½¿ç”¨ä¸‰åˆ†æ”¯ç‹¬ç«‹è®­ç»ƒçš„è§£ç å™¨
        vae = load_trained_motion_decoder(
            root_checkpoint_path=root_checkpoint_path,
            trans_checkpoint_path=trans_checkpoint_path,
            pose_checkpoint_path=pose_checkpoint_path,
            device=device,
            latent_channels=128,
            motion_channels_per_person=motion_channels_per_person,
        )
        print("=" * 50)
        
        # ğŸ”¥ æ³¨æ„ï¼šè§£ç å™¨å·²è½¬æ¢ä¸ºfloat16ï¼Œä¸éœ€è¦å†è½¬æ¢ä¸ºbfloat16
        print("ğŸ”§ è¿åŠ¨è§£ç å™¨å·²è½¬æ¢ä¸ºfloat16ï¼Œè·³è¿‡bfloat16è½¬æ¢")
        
    else:
        # æ ‡å‡†æ¨¡å¼ï¼šä»åŸå§‹checkpointåŠ è½½æ ‡å‡†VAE
        print("\nğŸ“¥ åŠ è½½æ ‡å‡†VAE...")
        vae = CausalVideoAutoencoder.from_pretrained(ckpt_path, local_files_only=True)
        print("âœ… æ ‡å‡†VAEåŠ è½½å®Œæˆ")
        vae = vae.to(torch.bfloat16)

    # å…¶ä»–ç»„ä»¶æ­£å¸¸åŠ è½½
    transformer = create_transformer(ckpt_path, precision)

    # åŠ è½½scheduler
    if sampler == "from_checkpoint" or not sampler:
        try:
            scheduler = RectifiedFlowScheduler.from_pretrained(ckpt_path)
        except TypeError as e:
            print(f"è­¦å‘Š: RectifiedFlowScheduler.from_pretrained() å¤±è´¥: {e}")
            print("åˆ›å»ºé»˜è®¤scheduler")
            scheduler = RectifiedFlowScheduler()
    else:
        scheduler = RectifiedFlowScheduler(
            sampler=("Uniform" if sampler.lower() == "uniform" else "LinearQuadratic")
        )

    text_encoder = load_transformers_model_with_fallback(
        text_encoder_model_name_or_path,
        T5EncoderModel,
        subfolder="text_encoder"
    )

    patchifier = SymmetricPatchifier(patch_size=1)

    tokenizer = load_transformers_model_with_fallback(
        text_encoder_model_name_or_path,
        T5Tokenizer,
        subfolder="tokenizer"
    )

    transformer = transformer.to(device)
    vae = vae.to(device)
    text_encoder = text_encoder.to(device)

    # å®Œå…¨ä¸åŠ è½½æç¤ºå¢å¼ºç›¸å…³æ¨¡å‹
    prompt_enhancer_image_caption_model = None
    prompt_enhancer_image_caption_processor = None
    prompt_enhancer_llm_model = None
    prompt_enhancer_llm_tokenizer = None

    # è½¬æ¢ä¸ºbfloat16ï¼ˆè¿åŠ¨è§£ç å™¨å·²ä¸ºfloat16ï¼Œä¸éœ€è¦è½¬æ¢ï¼‰
    if not motion_mode:
        vae = vae.to(torch.bfloat16)
    text_encoder = text_encoder.to(torch.bfloat16)

    submodel_dict = {
        "transformer": transformer,
        "patchifier": patchifier,
        "text_encoder": text_encoder,
        "tokenizer": tokenizer,
        "scheduler": scheduler,
        "vae": vae,
        "prompt_enhancer_image_caption_model": prompt_enhancer_image_caption_model,
        "prompt_enhancer_image_caption_processor": prompt_enhancer_image_caption_processor,
        "prompt_enhancer_llm_model": prompt_enhancer_llm_model,
        "prompt_enhancer_llm_tokenizer": prompt_enhancer_llm_tokenizer,
        "allowed_inference_steps": allowed_inference_steps,
    }

    pipeline = LTXVideoPipeline(**submodel_dict)
    pipeline = pipeline.to(device)

    return pipeline


def create_latent_upsampler(latent_upsampler_model_path: str, device: str):
    if not latent_upsampler_model_path:
        raise ValueError("æ½œåœ¨ä¸Šé‡‡æ ·å™¨æ¨¡å‹è·¯å¾„æœªæä¾›")

    latent_upsampler_model_path = check_local_path(latent_upsampler_model_path, "æ½œåœ¨ä¸Šé‡‡æ ·å™¨æ¨¡å‹")
    try:
        latent_upsampler = LatentUpsampler.from_pretrained(latent_upsampler_model_path, local_files_only=True)
    except TypeError:
        print(f"è­¦å‘Š: LatentUpsampler.from_pretrained() å¯èƒ½ä¸æ”¯æŒ local_files_only å‚æ•°")
        print(f"å°è¯•ä»æœ¬åœ°æ–‡ä»¶ç›´æ¥åŠ è½½: {latent_upsampler_model_path}")
        latent_upsampler = LatentUpsampler.from_pretrained(latent_upsampler_model_path)

    latent_upsampler.to(device)
    latent_upsampler.eval()
    return latent_upsampler


def load_pipeline_config(pipeline_config: str):
    current_file = Path(__file__)

    path = None
    if os.path.isfile(current_file.parent / pipeline_config):
        path = current_file.parent / pipeline_config
    elif os.path.isfile(pipeline_config):
        path = pipeline_config
    else:
        raise ValueError(f"Pipeline config file {pipeline_config} does not exist")

    with open(path, "r") as f:
        config = yaml.safe_load(f)

    print("=== åŠ è½½çš„ç®¡é“é…ç½® ===")
    for key, value in config.items():
        print(f"{key}: {value}")
    print("======================")

    return config


@dataclass
class InferenceConfig:
    prompt: str = field(metadata={"help": "Prompt for the generation"})

    output_path: str = field(
        default_factory=lambda: Path(
            f"outputs/{datetime.today().strftime('%Y-%m-%d')}"
        ),
        metadata={"help": "Path to the folder to save the output video"},
    )

    pipeline_config: str = field(
        default="configs/ltxv-2b-0.9.8-distilled.yaml",
        metadata={"help": "Path to the pipeline config file"},
    )
    seed: int = field(
        default=171198, metadata={"help": "Random seed for the inference"}
    )
    height: int = field(
        default=704, metadata={"help": "Height of the output video frames"}
    )
    width: int = field(
        default=1216, metadata={"help": "Width of the output video frames"}
    )
    num_frames: int = field(
        default=121,
        metadata={"help": "Number of frames to generate in the output video"},
    )
    frame_rate: int = field(
        default=30, metadata={"help": "Frame rate for the output video"},
    )
    offload_to_cpu: bool = field(
        default=False, metadata={"help": "Offloading unnecessary computations to CPU."}
    )
    negative_prompt: str = field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        metadata={"help": "Negative prompt for undesired features"},
    )

    motion_mode: bool = field(
        default=True,
        metadata={"help": "æ˜¯å¦å¯ç”¨è¿åŠ¨æ¨ç†æ¨¡å¼ï¼Œç”Ÿæˆ69ç»´è¿åŠ¨å‚æ•°"},
    )

    motion_channels_per_person: int = field(
        default=69,
        metadata={"help": "æ¯äººè¿åŠ¨å‚æ•°é€šé“æ•°ï¼Œé»˜è®¤ä¸º69"},
    )

    # ä¸‰åˆ†æ”¯æ£€æŸ¥ç‚¹è·¯å¾„
    root_checkpoint_path: str = field(
        default="/hy-tmp/elastic_root_models/latest_root_checkpoint.pt",
        metadata={"help": "rootåˆ†æ”¯æ£€æŸ¥ç‚¹è·¯å¾„"},
    )

    trans_checkpoint_path: str = field(
        default="/hy-tmp/elastic_trans_models/latest_trans_checkpoint.pt",
        metadata={"help": "transåˆ†æ”¯æ£€æŸ¥ç‚¹è·¯å¾„"},
    )

    pose_checkpoint_path: str = field(
        default="/hy-tmp/elastic_pose_models/latest_pose_checkpoint.pt",
        metadata={"help": "poseåˆ†æ”¯æ£€æŸ¥ç‚¹è·¯å¾„"},
    )

    motion_target_frames: Optional[int] = field(
        default=None,
        metadata={"help": "ç›®æ ‡å¸§æ•°ï¼ˆè¿åŠ¨æ¨¡å¼å¿…éœ€ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œåˆ™è‡ªåŠ¨è®¡ç®—"},
    )

    save_motion_params_path: Optional[str] = field(
        default=None,
        metadata={"help": "ä¿å­˜è¿åŠ¨å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ï¼ˆå¯é€‰ï¼‰"},
    )

    enable_second_stage: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦å¯ç”¨ç¬¬äºŒé˜¶æ®µé«˜åˆ†è¾¨ç‡ä¼˜åŒ–ï¼Œé»˜è®¤å…³é—­"},
    )

    save_first_stage_video: bool = field(
        default=True,
        metadata={"help": "æ˜¯å¦ä¿å­˜ç¬¬ä¸€é˜¶æ®µä½åˆ†è¾¨ç‡è§†é¢‘ï¼Œé»˜è®¤å¼€å¯"},
    )

    first_stage_filename: str = field(
        default="first_stage_low_res_video.mp4",
        metadata={"help": "ç¬¬ä¸€é˜¶æ®µè§†é¢‘çš„æ–‡ä»¶å"},
    )

    input_media_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "Path to the input video (or image) to be modified using the video-to-video pipeline"
        },
    )

    image_cond_noise_scale: float = field(
        default=0.15,
        metadata={"help": "Amount of noise to add to the conditioned image"},
    )
    conditioning_media_paths: Optional[List[str]] = field(
        default=None,
        metadata={
            "help": "List of paths to conditioning media (images or videos). Each path will be used as a conditioning item."
        },
    )
    conditioning_strengths: Optional[List[float]] = field(
        default=None,
        metadata={
            "help": "List of conditioning strengths (between 0 and 1) for each conditioning item. Must match the number of conditioning items."
        },
    )
    conditioning_start_frames: Optional[List[int]] = field(
        default=None,
        metadata={
            "help": "List of frame indices where each conditioning item should be applied. Must match the number of conditioning items."
        },
    )

    output_format: str = field(
        default="pkl",
        metadata={"help": "è¾“å‡ºæ ¼å¼: pkl, npy, pt, é»˜è®¤pkl"}
    )


def decode_latents_to_motion_simple(
        latents: torch.FloatTensor,
        vae,
        target_frames: int,
        motion_channels_per_person: int = 69
) -> torch.FloatTensor:
    """ç®€åŒ–çš„latentsè§£ç ä¸ºè¿åŠ¨å‚æ•°"""
    batch_size, channels, T_compressed, H, W = latents.shape

    print(f"\nğŸ” è§£ç å‚æ•°:")
    print(f"  - latentså½¢çŠ¶: {latents.shape}")
    print(f"  - ç›®æ ‡å¸§æ•°: {target_frames}")

    try:
        # ğŸ”¥ åˆ›å»ºéšæœºtimestepï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        timestep = torch.rand(batch_size, device=latents.device, dtype=latents.dtype)
        print(f"  - åˆ›å»ºtimestep: {timestep.shape}, dtype: {timestep.dtype}")

        # ä½¿ç”¨vaeçš„decodeæ–¹æ³•
        if hasattr(vae, 'decode'):
            motion = vae.decode(
                latents=latents,
                target_shape=(batch_size, motion_channels_per_person, target_frames, 1, W),
                timestep=timestep,  # ğŸ”¥ æä¾›timestepå‚æ•°
                return_dict=False
            )
        else:
            # å°è¯•ç›´æ¥è°ƒç”¨
            motion = vae(
                latents=latents,
                target_frames=target_frames,
                timestep=timestep,  # ğŸ”¥ æä¾›timestepå‚æ•°
                return_dict=False
            )

        print(f"âœ… VAEè§£ç æˆåŠŸ")
        return motion

    except Exception as e:
        print(f"âŒ VAEè§£ç å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"VAEè§£ç å¤±è´¥: {e}")


def decode_69d_to_smpl_params(motion_69d):
    """å°†69ç»´è¿åŠ¨å‚æ•°è§£ç ä¸ºSMPLæ ¼å¼"""
    if isinstance(motion_69d, torch.Tensor):
        motion_cpu = motion_69d.cpu().float().detach()
        motion_np = motion_cpu.numpy()
    else:
        motion_np = motion_69d

    if motion_np.shape[1] != 69:
        raise ValueError(f"æœŸæœ›69ç»´ï¼Œä½†å¾—åˆ°{motion_np.shape[1]}ç»´")

    # åˆ†å‰²ä¸ºä¸‰éƒ¨åˆ†
    trans = motion_np[:, :3]  # å¹³ç§» (3ç»´)
    root_orient = motion_np[:, 3:6]  # æ ¹æ–¹å‘ (3ç»´)
    pose_body = motion_np[:, 6:69]  # èº«ä½“å§¿åŠ¿ (63ç»´)

    return {
        'trans': trans,
        'root_orient': root_orient,
        'pose_body': pose_body,
        'gender': 'neutral'
    }


def infer(config: InferenceConfig):
    print("=" * 70)
    print("ğŸš€ å¼€å§‹æ¨ç† - ä½¿ç”¨ä¸‰åˆ†æ”¯ç‹¬ç«‹è®­ç»ƒçš„è¿åŠ¨è§£ç å™¨")
    print("=" * 70)

    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  è¿åŠ¨æ¨¡å¼: {'å¼€å¯' if config.motion_mode else 'å…³é—­'}")
    if config.motion_mode:
        print(f"  æ¯äººè¿åŠ¨é€šé“æ•°: {config.motion_channels_per_person}")
        print(f"  ç›®æ ‡å¸§æ•°: {config.motion_target_frames or 'è‡ªåŠ¨è®¡ç®—'}")
        print(f"  rootæ£€æŸ¥ç‚¹: {config.root_checkpoint_path}")
        print(f"  transæ£€æŸ¥ç‚¹: {config.trans_checkpoint_path}")
        print(f"  poseæ£€æŸ¥ç‚¹: {config.pose_checkpoint_path}")
        print(f"  ä¿å­˜è·¯å¾„: {config.save_motion_params_path or 'ä¸ä¿å­˜å•ç‹¬æ–‡ä»¶'}")

    print(f"  ç¬¬äºŒé˜¶æ®µ: {'å¼€å¯' if config.enable_second_stage else 'å…³é—­'}")
    print(f"  è¾“å‡ºæ ¼å¼: {config.output_format}")
    print(f"  æç¤º: {config.prompt[:50]}...")
    print(f"  åˆ†è¾¨ç‡: {config.height}x{config.width}x{config.num_frames}")
    print("=" * 70)

    if config.output_path:
        output_dir = Path(config.output_path)
    else:
        output_dir = Path(f"outputs/{datetime.today().strftime('%Y-%m-%d')}")

    pipeline_config = load_pipeline_config(config.pipeline_config)

    ltxv_model_name_or_path = pipeline_config["checkpoint_path"]

    ltxv_model_path = check_local_path(ltxv_model_name_or_path, "LTX-Videoä¸»æ¨¡å‹")
    print(f"ä¸»æ¨¡å‹è·¯å¾„: {ltxv_model_path}")

    spatial_upscaler_model_name_or_path = None
    spatial_upscaler_model_path = None

    if config.enable_second_stage and not config.motion_mode:
        spatial_upscaler_model_name_or_path = pipeline_config.get(
            "spatial_upscaler_model_path"
        )
        if spatial_upscaler_model_name_or_path:
            spatial_upscaler_model_path = check_local_path(
                spatial_upscaler_model_name_or_path,
                "ç©ºé—´ä¸Šé‡‡æ ·å™¨æ¨¡å‹"
            )
            print(f"ä¸Šé‡‡æ ·å™¨è·¯å¾„: {spatial_upscaler_model_path}")
        else:
            raise ValueError(
                "å¯ç”¨ç¬¬äºŒé˜¶æ®µéœ€è¦é…ç½®ç©ºé—´ä¸Šé‡‡æ ·å™¨æ¨¡å‹è·¯å¾„ (spatial_upscaler_model_path)ï¼Œä½†æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°"
            )
    elif config.enable_second_stage and config.motion_mode:
        print("æ³¨æ„: è¿åŠ¨æ¨¡å¼ä¸æ”¯æŒç¬¬äºŒé˜¶æ®µï¼Œå°†å¿½ç•¥å¯ç”¨ç¬¬äºŒé˜¶æ®µçš„è®¾ç½®")
    elif not config.enable_second_stage:
        print("ç¬¬äºŒé˜¶æ®µå·²ç¦ç”¨ï¼Œè·³è¿‡ç©ºé—´ä¸Šé‡‡æ ·å™¨åŠ è½½")

    conditioning_media_paths = config.conditioning_media_paths
    conditioning_strengths = config.conditioning_strengths
    conditioning_start_frames = config.conditioning_start_frames

    if conditioning_media_paths:
        if not conditioning_strengths:
            conditioning_strengths = [1.0] * len(conditioning_media_paths)
        if not conditioning_start_frames:
            raise ValueError(
                "If `conditioning_media_paths` is provided, "
                "`conditioning_start_frames` must also be provided"
            )
        if len(conditioning_media_paths) != len(conditioning_strengths) or len(
                conditioning_media_paths
        ) != len(conditioning_start_frames):
            raise ValueError(
                "`conditioning_media_paths`, `conditioning_strengths`, "
                "and `conditioning_start_frames` must have the same length"
            )
        if any(s < 0 or s > 1 for s in conditioning_strengths):
            raise ValueError("All conditioning strengths must be between 0 and 1")
        if any(f < 0 or f >= config.num_frames for f in conditioning_start_frames):
            raise ValueError(
                f"All conditioning start frames must be between 0 and {config.num_frames - 1}"
            )

    seed_everething(config.seed)
    if config.offload_to_cpu and not torch.cuda.is_available():
        logger.warning(
            "offload_to_cpu is set to True, but offloading will not occur since the model is already running on CPU."
        )
        offload_to_cpu = False
    else:
        offload_to_cpu = config.offload_to_cpu and get_total_gpu_memory() < 30

    output_dir = (
        Path(config.output_path)
        if config.output_path
        else Path(f"outputs/{datetime.today().strftime('%Y-%m-%d')}")
    )
    output_dir.mkdir(parents=True, exist_ok=True)

    height_padded = ((config.height - 1) // 32 + 1) * 32
    width_padded = ((config.width - 1) // 32 + 1) * 32
    num_frames_padded = ((config.num_frames - 2) // 8 + 1) * 8 + 1

    padding = calculate_padding(
        config.height, config.width, height_padded, width_padded
    )

    logger.warning(
        f"è°ƒæ•´åçš„ç»´åº¦: {height_padded}x{width_padded}x{num_frames_padded}"
    )
    print(f"ğŸ“ ç»´åº¦è°ƒæ•´:")
    print(f"  åŸå§‹: {config.height}x{config.width}x{config.num_frames}")
    print(f"  è°ƒæ•´å: {height_padded}x{width_padded}x{num_frames_padded}")

    device = get_device()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    prompt_enhancement_words_threshold = pipeline_config[
        "prompt_enhancement_words_threshold"
    ]

    prompt_word_count = len(config.prompt.split())
    enhance_prompt = (
            prompt_enhancement_words_threshold > 0
            and prompt_word_count < prompt_enhancement_words_threshold
    )

    if prompt_enhancement_words_threshold > 0 and not enhance_prompt:
        logger.info(
            f"Prompt has {prompt_word_count} words, which exceeds the threshold of {prompt_enhancement_words_threshold}. Prompt enhancement disabled."
        )

    # å¼ºåˆ¶å…³é—­æç¤ºå¢å¼º
    enhance_prompt = False
    print("æç¤ºå¢å¼ºåŠŸèƒ½å·²å¼ºåˆ¶å…³é—­")

    precision = pipeline_config["precision"]
    text_encoder_model_name_or_path = pipeline_config["text_encoder_model_name_or_path"]
    sampler = pipeline_config.get("sampler", None)
    prompt_enhancer_image_caption_model_name_or_path = None
    prompt_enhancer_llm_model_name_or_path = None

    print("\n" + "=" * 50)
    print("æ£€æŸ¥æ‰€æœ‰æ¨¡å‹è·¯å¾„")
    print("=" * 50)
    text_encoder_model_name_or_path = check_local_path(text_encoder_model_name_or_path, "æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹")
    print("æç¤ºå¢å¼ºåŠŸèƒ½å·²å…³é—­ï¼Œè·³è¿‡ç›¸å…³æ¨¡å‹æ£€æŸ¥")
    print("=" * 50)
    print("æ‰€æœ‰æ¨¡å‹è·¯å¾„æ£€æŸ¥å®Œæˆ")
    print("=" * 50 + "\n")

    print("å¼€å§‹åˆ›å»ºç®¡é“...")
    pipeline = create_ltx_video_pipeline(
        ckpt_path=ltxv_model_path,
        precision=precision,
        text_encoder_model_name_or_path=text_encoder_model_name_or_path,
        sampler=sampler,
        device=device,
        enhance_prompt=enhance_prompt,
        prompt_enhancer_image_caption_model_name_or_path=None,
        prompt_enhancer_llm_model_name_or_path=None,
        motion_mode=config.motion_mode,
        motion_channels_per_person=config.motion_channels_per_person,
        # ä¸‰åˆ†æ”¯æ£€æŸ¥ç‚¹è·¯å¾„
        root_checkpoint_path=config.root_checkpoint_path,
        trans_checkpoint_path=config.trans_checkpoint_path,
        pose_checkpoint_path=config.pose_checkpoint_path,
    )
    print("âœ… ç®¡é“åˆ›å»ºå®Œæˆ")

    pipeline_type = pipeline_config.get("pipeline_type", None)
    is_multi_scale = pipeline_type == "multi-scale"

    if config.enable_second_stage and is_multi_scale and not config.motion_mode:
        if not spatial_upscaler_model_path:
            raise ValueError(
                "spatial upscaler model path is missing from pipeline config file and is required for multi-scale rendering"
            )
        print("åˆ›å»ºæ½œåœ¨ä¸Šé‡‡æ ·å™¨...")
        latent_upsampler = create_latent_upsampler(
            spatial_upscaler_model_path, pipeline.device
        )
        pipeline = LTXMultiScalePipeline(pipeline, latent_upsampler=latent_upsampler)
        print("å¤šå°ºåº¦ç®¡é“åˆ›å»ºå®Œæˆ")
    elif config.enable_second_stage and not is_multi_scale and not config.motion_mode:
        print("æ³¨æ„: å¯ç”¨ç¬¬äºŒé˜¶æ®µä½†é…ç½®æ–‡ä»¶ä¸­ pipeline_type ä¸æ˜¯ 'multi-scale'ï¼Œå°†ä½¿ç”¨å•å°ºåº¦ç®¡é“")
    elif config.motion_mode:
        print("è¿åŠ¨æ¨¡å¼å¯ç”¨ï¼Œä½¿ç”¨å•å°ºåº¦ç®¡é“ï¼ˆè¿åŠ¨æ¨¡å¼ä¸æ”¯æŒå¤šå°ºåº¦ï¼‰")
    else:
        print("ç¬¬äºŒé˜¶æ®µå·²ç¦ç”¨ï¼Œä½¿ç”¨å•å°ºåº¦ç®¡é“")

    media_item = None
    if config.input_media_path:
        media_item = load_media_file(
            media_path=config.input_media_path,
            height=config.height,
            width=config.width,
            max_frames=num_frames_padded,
            padding=padding,
        )

    conditioning_items = (
        prepare_conditioning(
            conditioning_media_paths=conditioning_media_paths,
            conditioning_strengths=conditioning_strengths,
            conditioning_start_frames=conditioning_start_frames,
            height=config.height,
            width=config.width,
            num_frames=config.num_frames,
            padding=padding,
            pipeline=pipeline,
        )
        if conditioning_media_paths
        else None
    )

    stg_mode = pipeline_config.get("stg_mode", "attention_values")
    del pipeline_config["stg_mode"]
    if stg_mode.lower() == "stg_av" or stg_mode.lower() == "attention_values":
        skip_layer_strategy = SkipLayerStrategy.AttentionValues
    elif stg_mode.lower() == "stg_as" or stg_mode.lower() == "attention_skip":
        skip_layer_strategy = SkipLayerStrategy.AttentionSkip
    elif stg_mode.lower() == "stg_r" or stg_mode.lower() == "residual":
        skip_layer_strategy = SkipLayerStrategy.Residual
    elif stg_mode.lower() == "stg_t" or stg_mode.lower() == "transformer_block":
        skip_layer_strategy = SkipLayerStrategy.TransformerBlock
    else:
        raise ValueError(f"Invalid spatiotemporal guidance mode: {stg_mode}")

    sample = {
        "prompt": config.prompt,
        "prompt_attention_mask": None,
        "negative_prompt": config.negative_prompt,
        "negative_prompt_attention_mask": None,
    }

    generator = torch.Generator(device=device).manual_seed(config.seed)

    single_scale_params = pipeline_config.get("first_pass", {}).copy()

    base_params = {}
    for key, value in pipeline_config.items():
        if key not in ["first_pass", "second_pass", "downscale_factor", "pipeline_type"]:
            base_params[key] = value

    if single_scale_params:
        base_params.update(single_scale_params)

    required_params = ["timesteps", "guidance_scale", "stg_scale", "rescaling_scale"]
    for param in required_params:
        if param not in base_params:
            if param == "timesteps":
                with safe_open(ltxv_model_path, framework="pt") as f:
                    metadata = f.metadata()
                    config_str = metadata.get("config")
                    configs = json.loads(config_str)
                    allowed_inference_steps = configs.get("allowed_inference_steps", None)
                    if allowed_inference_steps:
                        base_params[param] = allowed_inference_steps
                    else:
                        base_params[param] = [1.0, 0.9937, 0.9875, 0.9812, 0.975, 0.9094, 0.725, 0.4219]
            elif param == "guidance_scale":
                base_params[param] = 1
            elif param == "stg_scale":
                base_params[param] = 0
            elif param == "rescaling_scale":
                base_params[param] = 1

    if "timesteps" in base_params:
        base_params["num_inference_steps"] = len(base_params["timesteps"])

    print(f"\nâš™ï¸ ç”Ÿæˆå‚æ•°:")
    print(f"  - timesteps: {base_params.get('timesteps', 'æœªè®¾ç½®')}")
    print(f"  - guidance_scale: {base_params.get('guidance_scale', 'æœªè®¾ç½®')}")
    print(f"  - stg_scale: {base_params.get('stg_scale', 'æœªè®¾ç½®')}")
    print(f"  - æ¨ç†æ­¥æ•°: {base_params.get('num_inference_steps', 'æœªè®¾ç½®')}")

    print("\n" + "=" * 70)
    print("å¼€å§‹ç”Ÿæˆ...")
    print("=" * 70)

    if config.motion_mode:
        print("\n" + "=" * 70)
        print("ğŸš€ å¯åŠ¨è¿åŠ¨æ¨ç†æ¨¡å¼ç”Ÿæˆ69ç»´è¿åŠ¨å‚æ•°")
        print("=" * 70)

        motion_target_frames = config.motion_target_frames
        if motion_target_frames is None:
            motion_target_frames = num_frames_padded
            print(f"ğŸ“ ç›®æ ‡å¸§æ•°è®¾ç½®ä¸ºè¾“å…¥å¸§æ•°: {motion_target_frames}")

        # ä½¿ç”¨pipelineçš„è¿åŠ¨æ¨¡å¼
        try:
            motion_output = pipeline.motion_inference(
                height=height_padded,
                width=width_padded,
                num_frames=num_frames_padded,
                frame_rate=config.frame_rate,
                **base_params,
                skip_layer_strategy=skip_layer_strategy,
                generator=generator,
                callback_on_step_end=None,
                **sample,
                media_items=media_item,
                conditioning_items=conditioning_items,
                is_video=True,
                vae_per_channel_normalize=True,
                image_cond_noise_scale=config.image_cond_noise_scale,
                mixed_precision=False,  # ğŸ”¥ ç¦ç”¨æ··åˆç²¾åº¦ä»¥é¿å…æ•°æ®ç±»å‹ä¸åŒ¹é…
                offload_to_cpu=offload_to_cpu,
                device=device,
                enhance_prompt=enhance_prompt,
                motion_channels_per_person=config.motion_channels_per_person,
                motion_target_frames=motion_target_frames,
                save_motion_params_path=config.save_motion_params_path,
            )

            print(f"\nâœ… è¿åŠ¨æ¨ç†å®Œæˆ")
            print(f"  - è¿åŠ¨å‚æ•°å½¢çŠ¶: {motion_output.motion_params.shape}")

            # ä¿å­˜è¿åŠ¨å‚æ•°ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
            if config.save_motion_params_path and hasattr(motion_output, 'motion_params'):
                save_motion_params(
                    motion_output=motion_output,
                    filepath=config.save_motion_params_path,
                    format="pt"
                )
                print(f"âœ… è¿åŠ¨å‚æ•°å·²ä¿å­˜åˆ°: {config.save_motion_params_path}")

        except AttributeError:
            print("âš ï¸ pipelineæ²¡æœ‰motion_inferenceæ–¹æ³•ï¼Œä½¿ç”¨æ‰‹åŠ¨è§£ç æ–¹å¼")
            # æ‰‹åŠ¨è§£ç æ–¹å¼
            original_output_type = "latent"

            print(f"\nğŸ¬ å¼€å§‹æ¨ç†...")
            result = pipeline(
                **base_params,
                skip_layer_strategy=skip_layer_strategy,
                generator=generator,
                output_type=original_output_type,
                callback_on_step_end=None,
                height=height_padded,
                width=width_padded,
                num_frames=num_frames_padded,
                frame_rate=config.frame_rate,
                **sample,
                media_items=media_item,
                conditioning_items=conditioning_items,
                is_video=True,
                vae_per_channel_normalize=True,
                image_cond_noise_scale=config.image_cond_noise_scale,
                mixed_precision=False,  # ğŸ”¥ ç¦ç”¨æ··åˆç²¾åº¦
                offload_to_cpu=offload_to_cpu,
                device=device,
                enhance_prompt=enhance_prompt,
            )

            if hasattr(result, 'images'):
                latents = result.images
            else:
                latents = result[0] if isinstance(result, tuple) else result

            print(f"\nâœ… æ¨ç†å®Œæˆ")
            print(f"  - ç”Ÿæˆçš„latentså½¢çŠ¶: {latents.shape}")

            print(f"ğŸ”§ è§£ç latentsä¸ºè¿åŠ¨å‚æ•°...")
            motion_params = decode_latents_to_motion_simple(
                latents=latents,
                vae=pipeline.vae,
                target_frames=motion_target_frames,
                motion_channels_per_person=config.motion_channels_per_person
            )

            print(f"âœ… è§£ç å®Œæˆ")
            print(f"  - è§£ç åçš„è¿åŠ¨å‚æ•°å½¢çŠ¶: {motion_params.shape}")

            motion_output = MotionVAEOutput(
                motion_params=motion_params,
                latents=latents,
                metadata={
                    'prompt': config.prompt,
                    'seed': config.seed,
                    'target_frames': motion_target_frames,
                    'channels_per_person': config.motion_channels_per_person,
                    'num_persons': motion_params.shape[4],
                    'height': config.height,
                    'width': config.width,
                    'num_frames': config.num_frames,
                }
            )

        persons_motion = motion_output.split_by_person()
        print(f"ğŸ‘¥ è¿åŠ¨å‚æ•°å·²æŒ‰{len(persons_motion)}äººåˆ†å‰²")

        print(f"\nğŸ’¾ ä¿å­˜è¿åŠ¨å‚æ•°...")
        for i in range(motion_output.motion_params.shape[0]):
            print(f"å¤„ç†ç¬¬{i + 1}ä¸ªbatch...")

            if len(persons_motion) > i:
                person1_motion_cpu = persons_motion[0][i].cpu() if i < persons_motion[0].shape[0] else \
                    persons_motion[0][0].cpu()
                person1_params = decode_69d_to_smpl_params(person1_motion_cpu)

                if len(persons_motion) > 1:
                    person2_motion_cpu = persons_motion[1][i].cpu() if i < persons_motion[1].shape[0] else \
                        persons_motion[1][0].cpu()
                    person2_params = decode_69d_to_smpl_params(person2_motion_cpu)
                else:
                    zeros_data = np.zeros_like(person1_motion_cpu.numpy())
                    person2_params = decode_69d_to_smpl_params(zeros_data)

                save_data = {
                    'person1': person1_params,
                    'person2': person2_params,
                    'mocap_framerate': float(config.frame_rate),
                    'frames': int(config.num_frames),

                    'metadata': {
                        'prompt': config.prompt,
                        'seed': config.seed,
                        'original_height': config.height,
                        'original_width': config.width,
                        'num_frames': config.num_frames,
                        'timestamp': datetime.now().isoformat(),
                        'generator': 'LTX-Video Motion Mode',
                        'motion_channels_per_person': config.motion_channels_per_person,
                        'motion_target_frames': motion_target_frames,
                    }
                }

                output_filename = get_unique_filename(
                    f"motion_{i}",
                    f".{config.output_format}",
                    prompt=config.prompt,
                    seed=config.seed,
                    resolution=(save_data['person1']['trans'].shape[0],
                                save_data['person1']['pose_body'].shape[1],
                                config.motion_channels_per_person),
                    dir=output_dir,
                )

                print(f"å‡†å¤‡ä¿å­˜åˆ°: {output_filename}")

                if config.output_format.lower() == 'pkl':
                    with open(output_filename, 'wb') as f:
                        pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)
                    print(f"âœ… è¿åŠ¨å‚æ•°å·²ä¿å­˜ä¸ºpklæ–‡ä»¶: {output_filename}")

                elif config.output_format.lower() == 'npy':
                    np.save(output_filename, save_data)
                    print(f"âœ… è¿åŠ¨å‚æ•°å·²ä¿å­˜ä¸ºnpyæ–‡ä»¶: {output_filename}")

                elif config.output_format.lower() == 'pt':
                    torch.save(save_data, output_filename)
                    print(f"âœ… è¿åŠ¨å‚æ•°å·²ä¿å­˜ä¸ºptæ–‡ä»¶: {output_filename}")

                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {config.output_format}")

            else:
                print(f"âš ï¸ è­¦å‘Š: persons_motioné•¿åº¦ä¸è¶³ï¼Œè·³è¿‡ç¬¬{i}ä¸ªbatch")

        print("\n" + "=" * 70)
        print("ğŸ‰ è¿åŠ¨æ¨ç†å®Œæˆ")
        print("=" * 70)
        print(f"è¿åŠ¨æ¨ç†æ¨¡å¼å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print("=" * 70)
        return

    else:
        print("\n" + "=" * 70)
        print("ä½¿ç”¨å•å°ºåº¦ç®¡é“è¿›è¡Œç”Ÿæˆ...")
        print("=" * 70)

        images = pipeline(
            **base_params,
            skip_layer_strategy=skip_layer_strategy,
            generator=generator,
            output_type="pt",
            callback_on_step_end=None,
            height=height_padded,
            width=width_padded,
            num_frames=num_frames_padded,
            frame_rate=config.frame_rate,
            **sample,
            media_items=media_item,
            conditioning_items=conditioning_items,
            is_video=True,
            vae_per_channel_normalize=True,
            image_cond_noise_scale=config.image_cond_noise_scale,
            mixed_precision=(precision == "mixed_precision"),
            offload_to_cpu=offload_to_cpu,
            device=device,
            enhance_prompt=enhance_prompt,
        ).images

        print(f"\nâœ… è§†é¢‘ç”Ÿæˆå®Œæˆ")
        print(f"  - ç”Ÿæˆçš„è§†é¢‘å½¢çŠ¶: {images.shape}")

        (pad_left, pad_right, pad_top, pad_bottom) = padding
        pad_bottom = -pad_bottom
        pad_right = -pad_right
        if pad_bottom == 0:
            pad_bottom = images.shape[3]
        if pad_right == 0:
            pad_right = images.shape[4]
        images = images[:, :, : config.num_frames, pad_top:pad_bottom, pad_left:pad_right]

        print(f"  - è£å‰ªåçš„è§†é¢‘å½¢çŠ¶: {images.shape}")

        print(f"\nğŸ’¾ ä¿å­˜å›¾åƒ/è§†é¢‘æ•°æ®...")
        for i in range(images.shape[0]):
            image_data = images[i].cpu()

            save_data = {
                'image_data': image_data,
                'prompt': config.prompt,
                'seed': config.seed,
                'original_height': config.height,
                'original_width': config.width,
                'num_frames': config.num_frames,
                'frame_rate': config.frame_rate,
                'negative_prompt': config.negative_prompt,
                'timestamp': datetime.now().isoformat(),
                'image_shape': image_data.shape,
                'image_dtype': str(image_data.dtype),
            }

            save_data['inference_params'] = {
                'timesteps': base_params.get('timesteps', []),
                'guidance_scale': base_params.get('guidance_scale', 1),
                'stg_scale': base_params.get('stg_scale', 0),
                'num_inference_steps': base_params.get('num_inference_steps', 0),
            }

            if config.enable_second_stage:
                output_filename = get_unique_filename(
                    f"video_output_stage2_{i}",
                    f".{config.output_format}",
                    prompt=config.prompt,
                    seed=config.seed,
                    resolution=(image_data.shape[2], image_data.shape[3], config.num_frames),
                    dir=output_dir,
                )
            else:
                output_filename = get_unique_filename(
                    f"video_output_stage1_{i}",
                    f".{config.output_format}",
                    prompt=config.prompt,
                    seed=config.seed,
                    resolution=(image_data.shape[2], image_data.shape[3], config.num_frames),
                    dir=output_dir,
                )

            if config.output_format.lower() == 'pkl':
                with open(output_filename, 'wb') as f:
                    pickle.dump(save_data, f, protocol=pickle.HIGHEST_PROTOCOL)
                print(f"âœ… å›¾åƒ/è§†é¢‘æ•°æ®å·²ä¿å­˜ä¸ºpklæ–‡ä»¶: {output_filename}")

            elif config.output_format.lower() == 'npy':
                np.save(output_filename, image_data.numpy())
                print(f"âœ… å›¾åƒ/è§†é¢‘æ•°æ®å·²ä¿å­˜ä¸ºnpyæ–‡ä»¶: {output_filename}")

            elif config.output_format.lower() == 'pt':
                torch.save(save_data, output_filename)
                print(f"âœ… å›¾åƒ/è§†é¢‘æ•°æ®å·²ä¿å­˜ä¸ºptæ–‡ä»¶: {output_filename}")

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {config.output_format}")

        print("\n" + "=" * 70)
        print("ğŸ‰ æ¨ç†å®Œæˆ")
        print("=" * 70)
        print(f"æ€»å…±ç”Ÿæˆ {images.shape[0]} ä¸ªå›¾åƒ/è§†é¢‘")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print("=" * 70)


def prepare_conditioning(
        conditioning_media_paths: List[str],
        conditioning_strengths: List[float],
        conditioning_start_frames: List[int],
        height: int,
        width: int,
        num_frames: int,
        padding: tuple[int, int, int, int],
        pipeline: LTXVideoPipeline,
) -> Optional[List[ConditioningItem]]:
    conditioning_items = []
    for path, strength, start_frame in zip(
            conditioning_media_paths, conditioning_strengths, conditioning_start_frames
    ):
        num_input_frames = orig_num_input_frames = get_media_num_frames(path)
        if hasattr(pipeline, "trim_conditioning_sequence") and callable(
                getattr(pipeline, "trim_conditioning_sequence")
        ):
            num_input_frames = pipeline.trim_conditioning_sequence(
                start_frame, orig_num_input_frames, num_frames
            )
        if num_input_frames < orig_num_input_frames:
            logger.warning(
                f"Trimming conditioning video {path} from {orig_num_input_frames} to {num_input_frames} frames."
            )

        media_tensor = load_media_file(
            media_path=path,
            height=height,
            width=width,
            max_frames=num_input_frames,
            padding=padding,
            just_crop=True,
        )
        conditioning_items.append(ConditioningItem(media_tensor, start_frame, strength))
    return conditioning_items


def get_media_num_frames(media_path: str) -> int:
    is_video = any(
        media_path.lower().endswith(ext) for ext in [".mp4", ".avi", ".mov", ".mkv"]
    )
    num_frames = 1
    if is_video:
        reader = imageio.get_reader(media_path)
        num_frames = reader.count_frames()
        reader.close()
    return num_frames


def load_media_file(
        media_path: str,
        height: int,
        width: int,
        max_frames: int,
        padding: tuple[int, int, int, int],
        just_crop: bool = False,
) -> torch.Tensor:
    is_video = any(
        media_path.lower().endswith(ext) for ext in [".mp4", ".avi", ".mov", ".mkv"]
    )
    if is_video:
        reader = imageio.get_reader(media_path)
        num_input_frames = min(reader.count_frames(), max_frames)

        frames = []
        for i in range(num_input_frames):
            frame = Image.fromarray(reader.get_data(i))
            frame_tensor = load_image_to_tensor_with_resize_and_crop(
                frame, height, width, just_crop=just_crop
            )
            frame_tensor = torch.nn.functional.pad(frame_tensor, padding)
            frames.append(frame_tensor)
        reader.close()

        media_tensor = torch.cat(frames, dim=2)
    else:
        media_tensor = load_image_to_tensor_with_resize_and_crop(
            media_path, height, width, just_crop=just_crop
        )
        media_tensor = torch.nn.functional.pad(media_tensor, padding)
    return media_tensor