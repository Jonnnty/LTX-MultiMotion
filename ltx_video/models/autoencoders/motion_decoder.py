#!/usr/bin/env python3
"""
å¼¹æ€§è¿åŠ¨è§£ç å™¨ - ä¸‰åˆ†æ”¯ç‹¬ç«‹ç½‘ç»œæ¶æ„
æ”¯æŒæ¸è¿›å¼å¢åŠ æ·±åº¦ï¼Œå®ç°è®­ç»ƒæ•°æ®çš„è¿‡æ‹Ÿåˆ
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple, Union, List, Dict
from einops import rearrange
import numpy as np

try:
    from diffusers.models.embeddings import PixArtAlphaCombinedTimestepSizeEmbeddings
except ImportError:
    PixArtAlphaCombinedTimestepSizeEmbeddings = None

from ltx_video.models.autoencoders.conv_nd_factory import make_conv_nd
from ltx_video.models.autoencoders.pixel_norm import PixelNorm
from ltx_video.models.autoencoders.pixel_shuffle import PixelShuffleND


class ElasticMotionDecoder(nn.Module):
    """
    å¼¹æ€§è¿åŠ¨è§£ç å™¨ - ä¸‰åˆ†æ”¯ç‹¬ç«‹ç½‘ç»œï¼Œæ”¯æŒæ·±åº¦æ‰©å±•

    è¾“å…¥: [batch, latent_channels, T_compressed, 1, n_persons]
    è¾“å‡º: [batch, 69, T_target, 1, n_persons]  # trans(3) + root_orient(3) + pose_body(63)

    ä¸‰ä¸ªå®Œå…¨ç‹¬ç«‹çš„ç½‘ç»œåˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯æ”¯æŒå¼¹æ€§å¢åŠ æ·±åº¦
    """

    def __init__(
            self,
            latent_channels: int = 128,
            motion_channels_per_person: int = 69,
            base_channels: int = 128,
            temporal_downscale_factor: int = 8,
            spatial_downscale_factor: int = 1,
            dims: int = 3,
            norm_layer: str = "group_norm",
            causal: bool = True,
            timestep_conditioning: bool = False,
            spatial_padding_mode: str = "zeros",
            # æ­£åˆ™åŒ–å‚æ•°
            dropout_rate: float = 0.1,
            use_weight_decay: bool = True,
            use_layer_norm: bool = False,
            use_stochastic_depth: bool = False,
            stochastic_depth_rate: float = 0.1,
            # è§£ç å™¨å—é…ç½®
            decoder_blocks: List[Tuple[str, int]] = [
                ("res_x", 2),
                ("compress_time", {"residual": True, "multiplier": 2}),
                ("compress_time", {"residual": True, "multiplier": 2}),
                ("compress_time", {"residual": True, "multiplier": 2}),
                ("res_x", 2),  # è¿™ç¬¬äºŒä¸ªres_xå—å°†è¢«å¼¹æ€§åŒ–
            ],
            # å¼¹æ€§å‚æ•°
            max_res_layers: int = 16,  # æ¯ä¸ªå¼¹æ€§å—æœ€å¤§å±‚æ•°
            initial_res_layers: int = 2,  # åˆå§‹å±‚æ•°
            use_elastic_depth: bool = True,  # æ˜¯å¦å¯ç”¨å¼¹æ€§æ·±åº¦
            **kwargs
    ):
        super().__init__()

        self.latent_channels = latent_channels
        self.motion_channels_per_person = motion_channels_per_person
        self.temporal_downscale_factor = temporal_downscale_factor
        self.spatial_downscale_factor = spatial_downscale_factor
        self.dims = dims
        self.causal = causal
        self.timestep_conditioning = timestep_conditioning

        # æ­£åˆ™åŒ–å‚æ•°
        self.dropout_rate = dropout_rate
        self.use_weight_decay = use_weight_decay
        self.use_layer_norm = use_layer_norm
        self.use_stochastic_depth = use_stochastic_depth
        self.stochastic_depth_rate = stochastic_depth_rate

        # å¼¹æ€§å‚æ•°
        self.max_res_layers = max_res_layers
        self.initial_res_layers = initial_res_layers
        self.use_elastic_depth = use_elastic_depth

        # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªè¿åŠ¨VAE
        self.is_motion_vae = True

        # ç¼©æ”¾å› å­
        self.scaling_factor = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))

        # è§£ç å™¨é…ç½®
        self.decoder_blocks_desc = decoder_blocks

        # æ·±åº¦å†å²è®°å½•
        self.depth_history = []
        self.loss_history = []
        self.current_depth = initial_res_layers  # å½“å‰æ¯ä¸ªåˆ†æ”¯çš„æ·±åº¦

        # ========== ä¸‰ä¸ªå®Œå…¨ç‹¬ç«‹çš„å¼¹æ€§è§£ç å™¨ ==========
        # transè§£ç å™¨ - è¾“å‡º3ç»´
        self.trans_decoder = ElasticMotionOnlyDecoder(
            dims=dims,
            in_channels=latent_channels,
            motion_channels_per_person=3,  # è¾“å‡º3ç»´å¹³ç§»
            blocks=decoder_blocks,
            base_channels=base_channels,
            norm_layer=norm_layer,
            causal=causal,
            timestep_conditioning=timestep_conditioning,
            spatial_padding_mode=spatial_padding_mode,
            dropout_rate=dropout_rate,
            use_layer_norm=use_layer_norm,
            use_stochastic_depth=use_stochastic_depth,
            stochastic_depth_rate=stochastic_depth_rate,
            max_res_layers=max_res_layers,
            initial_res_layers=initial_res_layers,
            use_elastic_depth=use_elastic_depth,
        )

        # root_orientè§£ç å™¨ - è¾“å‡º3ç»´
        self.root_decoder = ElasticMotionOnlyDecoder(
            dims=dims,
            in_channels=latent_channels,
            motion_channels_per_person=3,  # è¾“å‡º3ç»´æ ¹æ–¹å‘
            blocks=decoder_blocks,
            base_channels=base_channels,
            norm_layer=norm_layer,
            causal=causal,
            timestep_conditioning=timestep_conditioning,
            spatial_padding_mode=spatial_padding_mode,
            dropout_rate=dropout_rate,
            use_layer_norm=use_layer_norm,
            use_stochastic_depth=use_stochastic_depth,
            stochastic_depth_rate=stochastic_depth_rate,
            max_res_layers=max_res_layers,
            initial_res_layers=initial_res_layers,
            use_elastic_depth=use_elastic_depth,
        )

        # pose_bodyè§£ç å™¨ - è¾“å‡º63ç»´
        self.pose_decoder = ElasticMotionOnlyDecoder(
            dims=dims,
            in_channels=latent_channels,
            motion_channels_per_person=63,  # è¾“å‡º63ç»´èº«ä½“å§¿åŠ¿
            blocks=decoder_blocks,
            base_channels=base_channels,
            norm_layer=norm_layer,
            causal=causal,
            timestep_conditioning=timestep_conditioning,
            spatial_padding_mode=spatial_padding_mode,
            dropout_rate=dropout_rate,
            use_layer_norm=use_layer_norm,
            use_stochastic_depth=use_stochastic_depth,
            stochastic_depth_rate=stochastic_depth_rate,
            max_res_layers=max_res_layers,
            initial_res_layers=initial_res_layers,
            use_elastic_depth=use_elastic_depth,
        )
        # =========================================

        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"å¼¹æ€§è¿åŠ¨è§£ç å™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  åˆå§‹æ·±åº¦: {self.current_depth}å±‚/åˆ†æ”¯")
        print(f"  æœ€å¤§æ·±åº¦: {max_res_layers}å±‚/åˆ†æ”¯")

    def forward(
            self,
            latents: torch.FloatTensor,
            target_frames: int,
            timestep: Optional[torch.Tensor] = None,
            return_dict: bool = True,
            **kwargs
    ) -> Union[torch.FloatTensor, dict]:
        """
        è§£ç latentsä¸ºè¿åŠ¨å‚æ•° - ä¸‰åˆ†æ”¯ç‹¬ç«‹ç½‘ç»œ

        Args:
            latents: æ½œåœ¨è¡¨ç¤º [batch, latent_channels, T_compressed, 1, n_persons]
            target_frames: ç›®æ ‡å¸§æ•°
            timestep: å¯é€‰çš„æ—¶é—´æ­¥æ¡ä»¶
            return_dict: æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼

        Returns:
            è¿åŠ¨å‚æ•° [batch, 69, target_frames, 1, n_persons]
        """
        batch_size, channels, T_compressed, H, W = latents.shape
        n_persons = W

        # æ£€æŸ¥æ•°æ®ç±»å‹
        if hasattr(self.trans_decoder.conv_in, 'weight'):
            expected_dtype = self.trans_decoder.conv_in.weight.dtype
            if latents.dtype != expected_dtype:
                latents = latents.to(expected_dtype)

        # éªŒè¯è¾“å…¥å½¢çŠ¶
        assert H == 1, f"é«˜åº¦ç»´åº¦åº”ä¸º1ï¼Œå½“å‰ä¸º{H}"
        assert n_persons > 0, f"å®½åº¦ç»´åº¦ï¼ˆäººæ•°ï¼‰åº”å¤§äº0ï¼Œå½“å‰ä¸º{W}"
        assert channels == self.latent_channels, \
            f"è¾“å…¥é€šé“æ•°{channels} != é¢„æœŸé€šé“æ•°{self.latent_channels}"

        # è®¡ç®—ç›®æ ‡å½¢çŠ¶
        target_shape = (batch_size, self.motion_channels_per_person,
                        target_frames, 1, n_persons)

        # ========== ä¸‰ä¸ªåˆ†æ”¯åˆ†åˆ«å‰å‘ä¼ æ’­ ==========
        # 1. transè§£ç 
        trans_output = self.trans_decoder(
            latents,
            target_shape=(batch_size, 3, target_frames, 1, n_persons),
            timestep=timestep
        )
        
        # 2. root_orientè§£ç 
        root_output = self.root_decoder(
            latents,
            target_shape=(batch_size, 3, target_frames, 1, n_persons),
            timestep=timestep
        )
        
        # 3. pose_bodyè§£ç 
        pose_output = self.pose_decoder(
            latents,
            target_shape=(batch_size, 63, target_frames, 1, n_persons),
            timestep=timestep
        )
        
        # 4. æ‹¼æ¥ä¸‰ä¸ªéƒ¨åˆ†
        motion = torch.cat([trans_output, root_output, pose_output], dim=1)
        # =========================================

        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert motion.shape == target_shape, \
            f"è¾“å‡ºå½¢çŠ¶{motion.shape} != ç›®æ ‡å½¢çŠ¶{target_shape}"

        if return_dict:
            return {
                "motion_params": motion,
                "latents": latents,
                "target_frames": target_frames,
                "num_persons": n_persons,
                "trans_output": trans_output,
                "root_output": root_output,
                "pose_output": pose_output,
                "current_depth": self.get_current_depth(),
            }
        else:
            return motion

    # ========== å¼¹æ€§æ·±åº¦æ§åˆ¶æ–¹æ³• ==========
    
    def add_depth(self, num_layers: int = 1) -> bool:
        """
        ä¸ºæ‰€æœ‰ä¸‰ä¸ªåˆ†æ”¯å¢åŠ æ·±åº¦
        
        Args:
            num_layers: è¦å¢åŠ çš„å±‚æ•°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå¢åŠ æ·±åº¦
        """
        if not self.use_elastic_depth:
            print("è­¦å‘Š: å¼¹æ€§æ·±åº¦åŠŸèƒ½æœªå¯ç”¨")
            return False
        
        success = True
        
        # ä¸ºæ¯ä¸ªåˆ†æ”¯å¢åŠ æ·±åº¦
        trans_success = self.trans_decoder.add_res_layers(num_layers)
        root_success = self.root_decoder.add_res_layers(num_layers)
        pose_success = self.pose_decoder.add_res_layers(num_layers)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æ”¯éƒ½æˆåŠŸ
        success = trans_success and root_success and pose_success
        
        if success:
            # æ›´æ–°å½“å‰æ·±åº¦
            self.current_depth += num_layers
            self.depth_history.append(self.current_depth)
            print(f"å¼¹æ€§å¢åŠ æ·±åº¦æˆåŠŸ: å½“å‰æ·±åº¦ {self.current_depth} å±‚/åˆ†æ”¯")
        else:
            print(f"å¢åŠ æ·±åº¦å¤±è´¥ï¼Œå¯èƒ½å·²è¾¾åˆ°æœ€å¤§æ·±åº¦ {self.max_res_layers}")
        
        return success
    
    def get_current_depth(self) -> int:
        """è·å–å½“å‰æ·±åº¦ï¼ˆä¸‰ä¸ªåˆ†æ”¯çš„å¹³å‡æ·±åº¦ï¼‰"""
        depths = [
            self.trans_decoder.get_current_depth(),
            self.root_decoder.get_current_depth(),
            self.pose_decoder.get_current_depth()
        ]
        return int(sum(depths) / len(depths))
    
    def get_max_depth(self) -> int:
        """è·å–æœ€å¤§å…è®¸æ·±åº¦"""
        return self.max_res_layers
    
    def set_depth(self, target_depth: int) -> bool:
        """
        è®¾ç½®ç›®æ ‡æ·±åº¦
        
        Args:
            target_depth: ç›®æ ‡æ·±åº¦
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®
        """
        current_depth = self.get_current_depth()
        
        if target_depth == current_depth:
            return True
        
        if target_depth > self.max_res_layers:
            print(f"ç›®æ ‡æ·±åº¦ {target_depth} è¶…è¿‡æœ€å¤§å…è®¸æ·±åº¦ {self.max_res_layers}")
            return False
        
        if target_depth < 1:
            print(f"ç›®æ ‡æ·±åº¦ {target_depth} å¿…é¡»å¤§äº0")
            return False
        
        # è®¡ç®—éœ€è¦å¢åŠ çš„å±‚æ•°
        layers_to_add = target_depth - current_depth
        
        if layers_to_add > 0:
            return self.add_depth(layers_to_add)
        else:
            # å½“å‰ä¸æ”¯æŒå‡å°‘æ·±åº¦ï¼ˆä½†å¯ä»¥åç»­æ·»åŠ æ­¤åŠŸèƒ½ï¼‰
            print("å½“å‰ä¸æ”¯æŒå‡å°‘æ·±åº¦")
            return False
    
    def is_at_max_depth(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æ·±åº¦"""
        return self.get_current_depth() >= self.max_res_layers
    
    def record_loss(self, loss_value: float):
        """è®°å½•æŸå¤±å€¼"""
        self.loss_history.append(loss_value)
    
    def get_depth_history(self) -> List[int]:
        """è·å–æ·±åº¦å†å²"""
        return self.depth_history.copy()
    
    def get_loss_history(self) -> List[float]:
        """è·å–æŸå¤±å†å²"""
        return self.loss_history.copy()
    
    def should_increase_depth(self, patience: int = 3, min_improvement: float = 0.01) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å¢åŠ æ·±åº¦
        
        Args:
            patience: è€å¿ƒå€¼ï¼ˆè¿ç»­å¤šå°‘ä¸ªepochæ— æ˜¾è‘—æ”¹å–„ï¼‰
            min_improvement: æœ€å°æ”¹å–„é˜ˆå€¼
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥å¢åŠ æ·±åº¦
        """
        if len(self.loss_history) < patience + 1:
            return False
        
        # æ£€æŸ¥æœ€è¿‘patienceä¸ªepochçš„æŸå¤±æ”¹å–„æƒ…å†µ
        recent_losses = self.loss_history[-(patience + 1):]
        improvements = []
        
        for i in range(1, len(recent_losses)):
            improvement = recent_losses[i-1] - recent_losses[i]
            improvements.append(improvement)
        
        avg_improvement = sum(improvements) / len(improvements)
        
        # å¦‚æœå¹³å‡æ”¹å–„å°äºé˜ˆå€¼ï¼Œè€ƒè™‘å¢åŠ æ·±åº¦
        should_increase = avg_improvement < min_improvement
        
        if should_increase:
            print(f"å»ºè®®å¢åŠ æ·±åº¦: æœ€è¿‘{patience}ä¸ªepochå¹³å‡æ”¹å–„ {avg_improvement:.6f} < é˜ˆå€¼ {min_improvement}")
        
        return should_increase

    def decode(
            self,
            latents: torch.FloatTensor,
            target_shape: Tuple[int, int, int, int, int],
            timestep: Optional[torch.Tensor] = None,
            return_dict: bool = False,
            **kwargs
    ) -> torch.FloatTensor:
        """
        è§£ç æ¥å£ï¼Œå…¼å®¹vae_encode.pyä¸­çš„è°ƒç”¨æ–¹å¼
        """
        batch_size, channels, target_frames, H, W = target_shape
        assert H == 1, "ç›®æ ‡é«˜åº¦åº”ä¸º1"

        return self.forward(
            latents=latents,
            target_frames=target_frames,
            timestep=timestep,
            return_dict=return_dict
        )

    def split_by_person(self, motion_output: torch.FloatTensor) -> List[torch.FloatTensor]:
        """
        å°†è¿åŠ¨è¾“å‡ºæŒ‰äººåˆ†å‰²
        """
        batch_size, channels, T, H, n_persons = motion_output.shape

        persons_motion = []
        for i in range(n_persons):
            person_motion = motion_output[:, :, :, :, i:i + 1]
            person_motion = rearrange(person_motion, 'b c t 1 1 -> b t c')
            persons_motion.append(person_motion)

        return persons_motion

    def get_optimizer_params(self, learning_rate: float = 1e-4):
        """è·å–ä¼˜åŒ–å™¨å‚æ•°"""
        if self.use_weight_decay:
            weight_decay = 1e-3
        else:
            weight_decay = 1e-5

        return [
            {
                "params": self.parameters(),
                "lr": learning_rate,
                "weight_decay": weight_decay
            }
        ]

    @property
    def config(self):
        """è¿”å›é…ç½®ä¿¡æ¯"""
        import types
        return types.SimpleNamespace(
            _class_name="ElasticMotionDecoder",
            latent_channels=self.latent_channels,
            motion_channels_per_person=self.motion_channels_per_person,
            temporal_downscale_factor=self.temporal_downscale_factor,
            spatial_downscale_factor=self.spatial_downscale_factor,
            dims=self.dims,
            causal=self.causal,
            timestep_conditioning=self.timestep_conditioning,
            dropout_rate=self.dropout_rate,
            use_weight_decay=self.use_weight_decay,
            use_layer_norm=self.use_layer_norm,
            use_stochastic_depth=self.use_stochastic_depth,
            stochastic_depth_rate=self.stochastic_depth_rate,
            decoder_blocks=self.decoder_blocks_desc,
            # å¼¹æ€§å‚æ•°
            max_res_layers=self.max_res_layers,
            initial_res_layers=self.initial_res_layers,
            use_elastic_depth=self.use_elastic_depth,
            scaling_factor=self.scaling_factor.item()
        )

    def to_json_string(self) -> str:
        """è¿”å›JSONæ ¼å¼çš„é…ç½®å­—ç¬¦ä¸²"""
        import json
        config_dict = {
            "_class_name": "ElasticMotionDecoder",
            "latent_channels": self.latent_channels,
            "motion_channels_per_person": self.motion_channels_per_person,
            "temporal_downscale_factor": self.temporal_downscale_factor,
            "spatial_downscale_factor": self.spatial_downscale_factor,
            "dims": self.dims,
            "causal": self.causal,
            "timestep_conditioning": self.timestep_conditioning,
            "dropout_rate": self.dropout_rate,
            "use_weight_decay": self.use_weight_decay,
            "use_layer_norm": self.use_layer_norm,
            "use_stochastic_depth": self.use_stochastic_depth,
            "stochastic_depth_rate": self.stochastic_depth_rate,
            "decoder_blocks": self.decoder_blocks_desc,
            # å¼¹æ€§å‚æ•°
            "max_res_layers": self.max_res_layers,
            "initial_res_layers": self.initial_res_layers,
            "use_elastic_depth": self.use_elastic_depth,
            "scaling_factor": float(self.scaling_factor.item())
        }
        return json.dumps(config_dict, indent=2)

    @classmethod
    def from_config(cls, config: dict):
        """ä»é…ç½®åˆ›å»ºå®ä¾‹"""
        return cls(**config)

    @classmethod
    def from_pretrained(
            cls,
            pretrained_path: str,
            **kwargs
    ):
        """ä»é¢„è®­ç»ƒæƒé‡åŠ è½½"""
        import os
        from pathlib import Path

        pretrained_path = Path(pretrained_path)

        if pretrained_path.is_dir():
            config_path = pretrained_path / "config.json"
            weights_path = pretrained_path / "motion_decoder.pth"

            if not config_path.exists():
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

            import json
            with open(config_path, 'r') as f:
                config = json.load(f)

            model = cls.from_config(config)

            if weights_path.exists():
                state_dict = torch.load(weights_path, map_location='cpu')
                model.load_state_dict(state_dict)

        elif pretrained_path.is_file():
            if str(pretrained_path).endswith('.pth') or str(pretrained_path).endswith('.pt'):
                if 'config' not in kwargs:
                    raise ValueError("ä»æƒé‡æ–‡ä»¶åŠ è½½æ—¶éœ€è¦æä¾›configå‚æ•°")

                model = cls.from_config(kwargs['config'])
                state_dict = torch.load(pretrained_path, map_location='cpu')
                model.load_state_dict(state_dict)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {pretrained_path}")
        else:
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {pretrained_path}")

        return model

    def save_pretrained(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        from pathlib import Path

        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)

        config_path = save_path / "config.json"
        with open(config_path, 'w') as f:
            f.write(self.to_json_string())

        weights_path = save_path / "motion_decoder.pth"
        torch.save(self.state_dict(), weights_path)
        
        # ä¿å­˜æ·±åº¦å†å²
        if self.depth_history:
            depth_path = save_path / "depth_history.txt"
            with open(depth_path, 'w') as f:
                for depth in self.depth_history:
                    f.write(f"{depth}\n")
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")


class ElasticMotionOnlyDecoder(nn.Module):
    """
    å¼¹æ€§è¿åŠ¨è§£ç å™¨çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œæ”¯æŒæ·±åº¦æ‰©å±•
    """

    def __init__(
            self,
            dims: int = 3,
            in_channels: int = 128,
            motion_channels_per_person: int = 69,
            blocks: List[Tuple[str, int]] = [
                ("res_x", 2),
                ("compress_time", {"residual": True, "multiplier": 2}),
                ("compress_time", {"residual": True, "multiplier": 2}),
                ("compress_time", {"residual": True, "multiplier": 2}),
                ("res_x", 2),  # è¿™ç¬¬äºŒä¸ªres_xå—å°†è¢«å¼¹æ€§åŒ–
            ],
            base_channels: int = 128,
            norm_layer: str = "group_norm",
            causal: bool = True,
            timestep_conditioning: bool = False,
            spatial_padding_mode: str = "zeros",
            # æ–°å¢æ­£åˆ™åŒ–å‚æ•°
            dropout_rate: float = 0.1,
            use_layer_norm: bool = False,
            use_stochastic_depth: bool = False,
            stochastic_depth_rate: float = 0.1,
            # å¼¹æ€§å‚æ•°
            max_res_layers: int = 16,
            initial_res_layers: int = 2,
            use_elastic_depth: bool = True,
    ):
        super().__init__()

        self.dims = dims
        self.in_channels = in_channels
        self.motion_channels_per_person = motion_channels_per_person
        self.causal = causal
        self.timestep_conditioning = timestep_conditioning
        self.blocks_desc = blocks

        # æ­£åˆ™åŒ–å‚æ•°
        self.dropout_rate = dropout_rate
        self.use_layer_norm = use_layer_norm
        self.use_stochastic_depth = use_stochastic_depth
        self.stochastic_depth_rate = stochastic_depth_rate

        # å¼¹æ€§å‚æ•°
        self.max_res_layers = max_res_layers
        self.initial_res_layers = initial_res_layers
        self.use_elastic_depth = use_elastic_depth
        self.current_res_layers = initial_res_layers

        # æ ‡è®°å“ªäº›å—æ˜¯å¼¹æ€§çš„
        self.elastic_block_indices = []
        self.elastic_blocks = []

        # conv_in
        self.conv_in = make_conv_nd(
            dims,
            in_channels,
            base_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            causal=True,
            spatial_padding_mode=spatial_padding_mode,
        )

        # åœ¨conv_inåæ·»åŠ Dropout
        if dropout_rate > 0:
            self.dropout_after_conv_in = nn.Dropout3d(p=dropout_rate)
        else:
            self.dropout_after_conv_in = nn.Identity()

        # åˆ›å»ºä¸Šé‡‡æ ·å—
        self.up_blocks = nn.ModuleList([])
        output_channel = base_channels

        # é€†åºéå†å—é…ç½®
        for block_idx, (block_name, block_params) in enumerate(list(reversed(blocks))):
            input_channel = output_channel

            if isinstance(block_params, int):
                block_params = {"num_layers": block_params}

            # æ·»åŠ éšæœºæ·±åº¦
            if self.use_stochastic_depth and block_idx > 0:
                survival_prob = 1.0 - stochastic_depth_rate * (block_idx / len(blocks))
                block_params["survival_prob"] = survival_prob

            if block_name == "res_x":
                # å¦‚æœæ˜¯ç¬¬äºŒä¸ªres_xå—ï¼ˆç´¢å¼•ä¸º0å› ä¸ºåˆ—è¡¨æ˜¯é€†åºçš„ï¼‰ï¼Œåˆ™ä½¿ç”¨å¼¹æ€§ç‰ˆæœ¬
                # blocksä¸­çš„ç¬¬äºŒä¸ªres_xåœ¨é€†åºåæ˜¯ç¬¬ä¸€ä¸ª
                is_elastic = (block_idx == 0 and self.use_elastic_depth)
                
                if is_elastic:
                    # ä½¿ç”¨å¼¹æ€§å—
                    block = ElasticUNetMidBlock3D(
                        dims=dims,
                        in_channels=input_channel,
                        dropout=dropout_rate,
                        initial_layers=initial_res_layers,
                        max_layers=max_res_layers,
                        resnet_eps=1e-6,
                        resnet_groups=16,
                        norm_layer=norm_layer if not use_layer_norm else "layer_norm",
                        inject_noise=block_params.get("inject_noise", False),
                        timestep_conditioning=timestep_conditioning,
                        spatial_padding_mode=spatial_padding_mode,
                        use_elastic_depth=use_elastic_depth,
                    )
                    self.elastic_block_indices.append(len(self.up_blocks))
                    self.elastic_blocks.append(block)
                else:
                    # ä½¿ç”¨æ™®é€šå—
                    from ltx_video.models.autoencoders.causal_video_autoencoder import UNetMidBlock3D
                    block = UNetMidBlock3D(
                        dims=dims,
                        in_channels=input_channel,
                        dropout=dropout_rate,
                        num_layers=block_params.get("num_layers", 2),
                        resnet_eps=1e-6,
                        resnet_groups=16,
                        norm_layer=norm_layer if not use_layer_norm else "layer_norm",
                        inject_noise=block_params.get("inject_noise", False),
                        timestep_conditioning=timestep_conditioning,
                        spatial_padding_mode=spatial_padding_mode,
                    )
            elif block_name == "compress_time":
                multiplier = block_params.get("multiplier", 2)
                output_channel = output_channel // multiplier
                block = DepthToSpaceUpsample(
                    dims=dims,
                    in_channels=input_channel,
                    stride=(2, 1, 1),
                    residual=block_params.get("residual", False),
                    out_channels_reduction_factor=multiplier,
                    spatial_padding_mode=spatial_padding_mode,
                    dropout_rate=dropout_rate,
                )
            else:
                raise ValueError(f"æœªçŸ¥çš„å—ç±»å‹: {block_name}")

            self.up_blocks.append(block)

        # norm_out
        if use_layer_norm:
            from ltx_video.models.autoencoders.causal_video_autoencoder import LayerNorm
            self.conv_norm_out = LayerNorm(output_channel, eps=1e-6)
        elif norm_layer == "group_norm":
            if output_channel % 32 != 0:
                possible_groups = [16, 8, 4, 2, 1]
                selected_group = 1
                for g in possible_groups:
                    if output_channel % g == 0:
                        selected_group = g
                        break
                num_groups = selected_group
            else:
                num_groups = 32

            self.conv_norm_out = nn.GroupNorm(
                num_channels=output_channel, num_groups=num_groups, eps=1e-6
            )
        elif norm_layer == "pixel_norm":
            self.conv_norm_out = PixelNorm()
        elif norm_layer == "layer_norm":
            from ltx_video.models.autoencoders.causal_video_autoencoder import LayerNorm
            self.conv_norm_out = LayerNorm(output_channel, eps=1e-6)

        self.conv_act = nn.SiLU()

        # åœ¨æ¿€æ´»åæ·»åŠ Dropout
        if dropout_rate > 0:
            self.dropout_after_act = nn.Dropout3d(p=dropout_rate)
        else:
            self.dropout_after_act = nn.Identity()

        # conv_out - è¾“å‡ºè¿åŠ¨å‚æ•°
        self.conv_out = make_conv_nd(
            dims,
            output_channel,
            self.motion_channels_per_person,
            3,
            padding=1,
            causal=True,
            spatial_padding_mode=spatial_padding_mode,
        )

        # æ—¶é—´æ­¥æ¡ä»¶
        if timestep_conditioning:
            assert PixArtAlphaCombinedTimestepSizeEmbeddings is not None, \
                "éœ€è¦å®‰è£…diffusersä»¥ä½¿ç”¨æ—¶é—´æ­¥æ¡ä»¶"
            self.timestep_scale_multiplier = nn.Parameter(
                torch.tensor(1000.0, dtype=torch.float32)
            )
            self.last_time_embedder = PixArtAlphaCombinedTimestepSizeEmbeddings(
                output_channel * 2, 0
            )
            self.last_scale_shift_table = nn.Parameter(
                torch.randn(2, output_channel) / output_channel ** 0.5
            )

        self.gradient_checkpointing = False

    # ========== å¼¹æ€§æ·±åº¦æ§åˆ¶æ–¹æ³• ==========
    
    def add_res_layers(self, num_layers: int = 1) -> bool:
        """
        ä¸ºå¼¹æ€§å—å¢åŠ å±‚æ•°
        
        Args:
            num_layers: è¦å¢åŠ çš„å±‚æ•°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå¢åŠ 
        """
        if not self.use_elastic_depth:
            print("è­¦å‘Š: å¼¹æ€§æ·±åº¦åŠŸèƒ½æœªå¯ç”¨")
            return False
        
        success = True
        
        # ä¸ºæ‰€æœ‰å¼¹æ€§å—å¢åŠ å±‚æ•°
        for elastic_block in self.elastic_blocks:
            for _ in range(num_layers):
                if not elastic_block.add_layer():
                    success = False
                    break
        
        if success:
            self.current_res_layers += num_layers
            print(f"å¢åŠ å±‚æ•°æˆåŠŸ: å½“å‰ {self.current_res_layers} å±‚")
        else:
            print(f"å¢åŠ å±‚æ•°å¤±è´¥ï¼Œå¯èƒ½å·²è¾¾åˆ°æœ€å¤§å±‚æ•° {self.max_res_layers}")
        
        return success
    
    def get_current_depth(self) -> int:
        """è·å–å½“å‰æ·±åº¦"""
        return self.current_res_layers
    
    def get_max_depth(self) -> int:
        """è·å–æœ€å¤§æ·±åº¦"""
        return self.max_res_layers
    
    def set_depth(self, target_depth: int) -> bool:
        """
        è®¾ç½®ç›®æ ‡æ·±åº¦
        
        Args:
            target_depth: ç›®æ ‡æ·±åº¦
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®
        """
        if target_depth == self.current_res_layers:
            return True
        
        if target_depth > self.max_res_layers:
            print(f"ç›®æ ‡æ·±åº¦ {target_depth} è¶…è¿‡æœ€å¤§å…è®¸æ·±åº¦ {self.max_res_layers}")
            return False
        
        if target_depth < 1:
            print(f"ç›®æ ‡æ·±åº¦ {target_depth} å¿…é¡»å¤§äº0")
            return False
        
        # è®¡ç®—éœ€è¦å¢åŠ çš„å±‚æ•°
        layers_to_add = target_depth - self.current_res_layers
        
        if layers_to_add > 0:
            return self.add_res_layers(layers_to_add)
        else:
            # å½“å‰ä¸æ”¯æŒå‡å°‘æ·±åº¦
            print("å½“å‰ä¸æ”¯æŒå‡å°‘æ·±åº¦")
            return False
    
    def is_at_max_depth(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æ·±åº¦"""
        return self.current_res_layers >= self.max_res_layers

    def forward(
            self,
            sample: torch.FloatTensor,
            target_shape: Tuple[int, int, int, int, int],
            timestep: Optional[torch.Tensor] = None,
    ) -> torch.FloatTensor:
        batch_size = sample.shape[0]

        # é€šè¿‡conv_in
        sample = self.conv_in(sample, causal=self.causal)

        # åº”ç”¨Dropout
        if self.dropout_rate > 0 and self.training:
            sample = self.dropout_after_conv_in(sample)

        if self.timestep_conditioning:
            assert timestep is not None, "æ—¶é—´æ­¥æ¡ä»¶éœ€è¦æä¾›timestepå‚æ•°"
            
            # é¦–å…ˆç¡®ä¿ timestep æœ‰æ­£ç¡®çš„å½¢çŠ¶
            # å¦‚æœ timestep æ˜¯ 1D [B] æˆ– 5D [B, 1, 1, 1, 1]ï¼Œç¡®ä¿å®ƒæ˜¯ 5D
            if timestep.dim() == 1:  # [batch_size]
                timestep = timestep.view(-1, 1, 1, 1, 1)
            elif timestep.dim() == 5:  # [batch_size, 1, 1, 1, 1]
                # ç¡®ä¿ç¬¬äºŒç»´æ˜¯1
                if timestep.shape[1] != 1:
                    timestep = timestep.view(-1, 1, 1, 1, 1)
            else:
                # æ‰©å±•ç»´åº¦åˆ°5D
                while timestep.dim() < 5:
                    timestep = timestep.unsqueeze(-1)
                if timestep.shape[1] != 1:
                    timestep = timestep.view(timestep.shape[0], 1, 1, 1, 1)
            
            scaled_timestep = timestep * self.timestep_scale_multiplier

        # é€šè¿‡ä¸Šé‡‡æ ·å—
        for i, up_block in enumerate(self.up_blocks):
            if self.gradient_checkpointing and self.training:
                if self.timestep_conditioning and hasattr(up_block,
                                                          'timestep_conditioning') and up_block.timestep_conditioning:
                    sample = torch.utils.checkpoint.checkpoint(
                        up_block, sample, self.causal, scaled_timestep,
                        use_reentrant=False
                    )
                else:
                    sample = torch.utils.checkpoint.checkpoint(
                        up_block, sample, self.causal,
                        use_reentrant=False
                    )
            else:
                if self.timestep_conditioning and hasattr(up_block,
                                                          'timestep_conditioning') and up_block.timestep_conditioning:
                    sample = up_block(sample, causal=self.causal, timestep=scaled_timestep)
                else:
                    sample = up_block(sample, causal=self.causal)

        # norm_out
        sample = self.conv_norm_out(sample)

        # æ—¶é—´æ­¥æ¡ä»¶ï¼ˆåœ¨norm_outåæ·»åŠ ï¼‰
        if self.timestep_conditioning:
            embedded_timestep = self.last_time_embedder(
                timestep=scaled_timestep.flatten(),
                resolution=None,
                aspect_ratio=None,
                batch_size=sample.shape[0],
                hidden_dtype=sample.dtype,
            )
            embedded_timestep = embedded_timestep.view(
                batch_size, embedded_timestep.shape[-1], 1, 1, 1
            )
            ada_values = self.last_scale_shift_table[
                             None, ..., None, None, None
                         ] + embedded_timestep.reshape(
                batch_size,
                2,
                -1,
                embedded_timestep.shape[-3],
                embedded_timestep.shape[-2],
                embedded_timestep.shape[-1],
            )
            shift, scale = ada_values.unbind(dim=1)
            sample = sample * (1 + scale) + shift

        # æ¿€æ´»å’Œè¾“å‡º
        sample = self.conv_act(sample)

        # åº”ç”¨Dropoutåœ¨æ¿€æ´»å
        if self.dropout_rate > 0 and self.training:
            sample = self.dropout_after_act(sample)

        sample = self.conv_out(sample, causal=self.causal)

        return sample


class ElasticUNetMidBlock3D(nn.Module):
    """
    å¼¹æ€§UNetä¸­é—´å—ï¼Œæ”¯æŒåŠ¨æ€å¢åŠ æ®‹å·®å±‚å±‚æ•°
    ä¿®å¤ï¼šResnetBlock3Då‚æ•°åŒ¹é…é—®é¢˜å’Œæ—¶é—´æ­¥åµŒå…¥é—®é¢˜
    """
    def __init__(
        self,
        dims,
        in_channels,
        dropout=0.0,
        initial_layers=2,
        max_layers=16,
        resnet_eps=1e-6,
        resnet_groups=16,
        norm_layer="group_norm",
        inject_noise=False,
        timestep_conditioning=False,
        spatial_padding_mode="zeros",
        use_elastic_depth=True,
    ):
        super().__init__()
        
        from ltx_video.models.autoencoders.causal_video_autoencoder import ResnetBlock3D
        
        self.in_channels = in_channels
        self.dims = dims
        self.initial_layers = initial_layers
        self.max_layers = max_layers
        self.current_layers = initial_layers
        self.timestep_conditioning = timestep_conditioning
        self.use_elastic_depth = use_elastic_depth
        
        # æ—¶é—´æ­¥åµŒå…¥å™¨ï¼ˆç…§æ¬æ ‡å‡†UNetMidBlock3Dï¼‰
        if timestep_conditioning:
            assert PixArtAlphaCombinedTimestepSizeEmbeddings is not None, \
                "éœ€è¦å®‰è£…diffusersä»¥ä½¿ç”¨æ—¶é—´æ­¥æ¡ä»¶"
            self.time_embedder = PixArtAlphaCombinedTimestepSizeEmbeddings(
                in_channels * 4, 0
            )
        
        # åˆ›å»ºæ®‹å·®å—åˆ—è¡¨
        self.resnets = nn.ModuleList()
        
        # åˆå§‹åŒ–æŒ‡å®šæ•°é‡çš„å—
        for i in range(initial_layers):
            resnet = ResnetBlock3D(
                dims=dims,
                in_channels=in_channels,
                out_channels=in_channels,
                dropout=dropout,
                groups=resnet_groups,  # æ³¨æ„ï¼šResnetBlock3Dä½¿ç”¨groupså‚æ•°
                eps=resnet_eps,
                norm_layer=norm_layer,
                inject_noise=inject_noise,
                timestep_conditioning=timestep_conditioning,  # å¸ƒå°”å€¼ï¼Œä¸æ˜¯é€šé“æ•°
                spatial_padding_mode=spatial_padding_mode,
            )
            self.resnets.append(resnet)
        
        # æ¿€æ´»çŠ¶æ€æ ‡è®°
        self.active_layers = list(range(initial_layers))
        
    def add_layer(self) -> bool:
        """
        æ·»åŠ ä¸€ä¸ªæ–°çš„æ®‹å·®å±‚
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        if not self.use_elastic_depth:
            return False
        
        if len(self.resnets) >= self.max_layers:
            print(f"å·²è¾¾åˆ°æœ€å¤§å±‚æ•° {self.max_layers}")
            return False
        
        from ltx_video.models.autoencoders.causal_video_autoencoder import ResnetBlock3D
        
        # åˆ›å»ºæ–°å—
        new_resnet = ResnetBlock3D(
            dims=self.dims,
            in_channels=self.in_channels,
            out_channels=self.in_channels,
            dropout=0.0,  # æ–°å—æš‚æ—¶ä¸åŠ dropout
            groups=16,  # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
            eps=1e-6,
            norm_layer="group_norm",
            inject_noise=False,
            timestep_conditioning=self.timestep_conditioning,
            spatial_padding_mode="zeros",
        )
        
        # æ·»åŠ åˆ°åˆ—è¡¨
        self.resnets.append(new_resnet)
        
        # æ›´æ–°æ¿€æ´»å±‚åˆ—è¡¨
        self.active_layers.append(len(self.resnets) - 1)
        self.current_layers = len(self.active_layers)
        
        # åˆå§‹åŒ–æ–°å—çš„æƒé‡ï¼ˆä¿æŒæ—§å—ä¸å˜ï¼‰
        for name, param in new_resnet.named_parameters():
            if 'weight' in name and param.dim() >= 2:
                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='linear')
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)
        
        print(f"å¼¹æ€§å—æ·»åŠ æ–°å±‚: å½“å‰ {self.current_layers}/{self.max_layers} å±‚")
        return True
    
    def get_current_depth(self) -> int:
        """è·å–å½“å‰æ·±åº¦"""
        return self.current_layers

    def forward(self, hidden_states, causal=True, timestep=None):
        """å‰å‘ä¼ æ’­ï¼Œæ”¯æŒå¼¹æ€§æ·±åº¦ï¼Œç…§æ¬æ ‡å‡†UNetMidBlock3Dçš„æ—¶é—´æ­¥å¤„ç†"""
        # æ—¶é—´æ­¥åµŒå…¥å¤„ç†ï¼ˆç…§æ¬æ ‡å‡†ä»£ç ï¼‰
        timestep_embed = None
        if self.timestep_conditioning:
            assert timestep is not None, "æ—¶é—´æ­¥æ¡ä»¶éœ€è¦æä¾›timestepå‚æ•°"
            batch_size = hidden_states.shape[0]
            
            # ä½¿ç”¨time_embedderå¤„ç†æ—¶é—´æ­¥
            timestep_embed = self.time_embedder(
                timestep=timestep.flatten(),  # å±•å¹³æ—¶é—´æ­¥
                resolution=None,
                aspect_ratio=None,
                batch_size=batch_size,
                hidden_dtype=hidden_states.dtype,
            )
            # å°†åµŒå…¥å‘é‡reshapeä¸º5D: [batch_size, channels, 1, 1, 1]
            timestep_embed = timestep_embed.view(
                batch_size, timestep_embed.shape[-1], 1, 1, 1
            )
        
        output_states = hidden_states
        
        # ä½¿ç”¨æ‰€æœ‰æ¿€æ´»çš„å±‚
        for i, resnet in enumerate(self.resnets):
            if i in self.active_layers:
                # ç›´æ¥ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œä¸å†ä½¿ç”¨alphaå‚æ•°
                output_states = resnet(output_states, causal=causal, timestep=timestep_embed)
        
        return output_states


class DepthToSpaceUpsample(nn.Module):
    """ä¸causal_video_autoencoder.pyä¸­çš„DepthToSpaceUpsampleç›¸åŒ"""

    def __init__(
            self,
            dims,
            in_channels,
            stride,
            residual=False,
            out_channels_reduction_factor=1,
            spatial_padding_mode="zeros",
            dropout_rate=0.0,
    ):
        super().__init__()
        import numpy as np

        self.stride = stride
        self.out_channels = (
                np.prod(stride) * in_channels // out_channels_reduction_factor
        )

        self.conv = make_conv_nd(
            dims=dims,
            in_channels=in_channels,
            out_channels=self.out_channels,
            kernel_size=3,
            stride=1,
            causal=True,
            spatial_padding_mode=spatial_padding_mode,
        )
        self.pixel_shuffle = PixelShuffleND(dims=dims, upscale_factors=stride)
        self.residual = residual
        self.out_channels_reduction_factor = out_channels_reduction_factor

        if dropout_rate > 0:
            self.dropout = nn.Dropout3d(p=dropout_rate)
        else:
            self.dropout = nn.Identity()

    def forward(self, x, causal: bool = True):
        import numpy as np

        if self.residual:
            x_in = self.pixel_shuffle(x)
            num_repeat = np.prod(self.stride) // self.out_channels_reduction_factor
            x_in = x_in.repeat(1, num_repeat, 1, 1, 1)
            if self.stride[0] == 2:
                x_in = x_in[:, :, 1:, :, :]

        x = self.conv(x, causal=causal)
        x = self.dropout(x)
        x = self.pixel_shuffle(x)

        if self.stride[0] == 2:
            x = x[:, :, 1:, :, :]

        if self.residual:
            x = x + x_in

        return x


def create_elastic_motion_decoder_config(
        latent_channels: int = 128,
        motion_channels_per_person: int = 69,
        temporal_downscale_factor: int = 8,
        spatial_downscale_factor: int = 1,
        base_channels: int = 128,
        causal: bool = True,
        timestep_conditioning: bool = False,
        # æ­£åˆ™åŒ–å‚æ•°
        dropout_rate: float = 0.1,
        use_weight_decay: bool = True,
        use_layer_norm: bool = False,
        use_stochastic_depth: bool = False,
        stochastic_depth_rate: float = 0.1,
        # å¼¹æ€§å‚æ•°
        max_res_layers: int = 16,
        initial_res_layers: int = 2,
        use_elastic_depth: bool = True,
        decoder_blocks: List[Tuple[str, int]] = [
            ("res_x", 2),
            ("compress_time", {"residual": True, "multiplier": 2}),
            ("compress_time", {"residual": True, "multiplier": 2}),
            ("compress_time", {"residual": True, "multiplier": 2}),
            ("res_x", 2),  # è¿™ç¬¬äºŒä¸ªres_xå—å°†è¢«å¼¹æ€§åŒ–
        ]
) -> dict:
    """
    åˆ›å»ºå¼¹æ€§è¿åŠ¨è§£ç å™¨é…ç½®

    Returns:
        é…ç½®å­—å…¸
    """
    return {
        "_class_name": "ElasticMotionDecoder",
        "latent_channels": latent_channels,
        "motion_channels_per_person": motion_channels_per_person,
        "temporal_downscale_factor": temporal_downscale_factor,
        "spatial_downscale_factor": spatial_downscale_factor,
        "base_channels": base_channels,
        "dims": 3,
        "norm_layer": "group_norm",
        "causal": causal,
        "timestep_conditioning": timestep_conditioning,
        "spatial_padding_mode": "zeros",
        "dropout_rate": dropout_rate,
        "use_weight_decay": use_weight_decay,
        "use_layer_norm": use_layer_norm,
        "use_stochastic_depth": use_stochastic_depth,
        "stochastic_depth_rate": stochastic_depth_rate,
        # å¼¹æ€§å‚æ•°
        "max_res_layers": max_res_layers,
        "initial_res_layers": initial_res_layers,
        "use_elastic_depth": use_elastic_depth,
        "decoder_blocks": decoder_blocks
    }


class ProgressiveTrainer:
    """
    æ¸è¿›å¼è®­ç»ƒå™¨ï¼Œè‡ªåŠ¨ç®¡ç†å¼¹æ€§æ·±åº¦å¢åŠ 
    """
    def __init__(
        self,
        model: ElasticMotionDecoder,
        train_loader,
        val_loader=None,
        learning_rate: float = 1e-4,
        patience: int = 3,
        min_improvement: float = 0.01,
        target_loss: float = 1e-4,
        max_epochs_per_depth: int = 10,
        warmup_epochs: int = 5,
    ):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.learning_rate = learning_rate
        self.patience = patience
        self.min_improvement = min_improvement
        self.target_loss = target_loss
        self.max_epochs_per_depth = max_epochs_per_depth
        self.warmup_epochs = warmup_epochs
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.get_optimizer_params(learning_rate)[0]['params'],
            lr=learning_rate
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()
        
        # è®­ç»ƒå†å²
        self.train_loss_history = []
        self.val_loss_history = [] if val_loader else []
        self.epoch_counter = 0
        
    def train_until_target(self, max_total_epochs: int = 100):
        """
        è®­ç»ƒç›´åˆ°è¾¾åˆ°ç›®æ ‡æŸå¤±æˆ–æœ€å¤§epochæ•°
        
        Args:
            max_total_epochs: æœ€å¤§æ€»epochæ•°
            
        Returns:
            bool: æ˜¯å¦è¾¾åˆ°ç›®æ ‡æŸå¤±
        """
        print("å¼€å§‹æ¸è¿›å¼è®­ç»ƒ...")
        print(f"åˆå§‹æ·±åº¦: {self.model.get_current_depth()}å±‚/åˆ†æ”¯")
        print(f"ç›®æ ‡æŸå¤±: {self.target_loss}")
        
        stagnation_counter = 0
        best_loss = float('inf')
        
        while self.epoch_counter < max_total_epochs:
            epoch = self.epoch_counter + 1
            print(f"\n=== Epoch {epoch}/{max_total_epochs} ===")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self._train_epoch()
            self.train_loss_history.append(train_loss)
            self.model.record_loss(train_loss)
            
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            
            # éªŒè¯ï¼ˆå¦‚æœæœ‰éªŒè¯é›†ï¼‰
            if self.val_loader is not None:
                val_loss = self._validate()
                self.val_loss_history.append(val_loss)
                print(f"éªŒè¯æŸå¤±: {val_loss:.6f}")
                current_loss = val_loss
            else:
                current_loss = train_loss
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if current_loss <= self.target_loss:
                print(f"\nğŸ‰ è¾¾åˆ°ç›®æ ‡æŸå¤± {self.target_loss}!")
                print(f"æœ€ç»ˆæ·±åº¦: {self.model.get_current_depth()}å±‚/åˆ†æ”¯")
                return True
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢åŠ æ·±åº¦ï¼ˆwarmupåï¼‰
            if epoch > self.warmup_epochs:
                should_increase = False
                
                # æ–¹æ³•1: ä½¿ç”¨æ¨¡å‹çš„è‡ªåŠ¨åˆ¤æ–­
                should_increase = self.model.should_increase_depth(
                    patience=self.patience,
                    min_improvement=self.min_improvement
                )
                
                # æ–¹æ³•2: ç®€å•åˆ¤æ–­ - é•¿æ—¶é—´æ— æ”¹å–„
                if not should_increase and len(self.train_loss_history) >= self.patience + 1:
                    recent_losses = self.train_loss_history[-(self.patience + 1):]
                    avg_improvement = sum(recent_losses[i-1] - recent_losses[i] 
                                        for i in range(1, len(recent_losses))) / (len(recent_losses) - 1)
                    if avg_improvement < self.min_improvement * 0.5:
                        should_increase = True
                        print(f"é•¿æ—¶é—´æ— æ”¹å–„ï¼Œå»ºè®®å¢åŠ æ·±åº¦")
                
                # å¢åŠ æ·±åº¦
                if should_increase:
                    if self.model.is_at_max_depth():
                        print(f"å·²è¾¾åˆ°æœ€å¤§æ·±åº¦ {self.model.get_max_depth()}ï¼Œæ— æ³•ç»§ç»­å¢åŠ ")
                    else:
                        print("æ­£åœ¨å¢åŠ æ·±åº¦...")
                        success = self.model.add_depth(num_layers=1)
                        if success:
                            print(f"æ·±åº¦å¢åŠ åˆ° {self.model.get_current_depth()}å±‚/åˆ†æ”¯")
                            stagnation_counter = 0
                            best_loss = float('inf')
                            
                            # è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼‰
                            self._adjust_learning_rate(0.8)  # æš‚æ—¶é™ä½å­¦ä¹ ç‡
                        else:
                            print("å¢åŠ æ·±åº¦å¤±è´¥")
            
            # æ›´æ–°æœ€ä½³æŸå¤±å’Œåœæ»è®¡æ•°å™¨
            if current_loss < best_loss - self.min_improvement:
                best_loss = current_loss
                stagnation_counter = 0
            else:
                stagnation_counter += 1
            
            # é•¿æ—¶é—´åœæ»ä½†æ— æ³•å¢åŠ æ·±åº¦æ—¶ï¼Œè€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡
            if stagnation_counter >= self.patience * 2 and self.model.is_at_max_depth():
                print("é•¿æ—¶é—´åœæ»ä¸”å·²è¾¾æœ€å¤§æ·±åº¦ï¼Œè°ƒæ•´å­¦ä¹ ç‡...")
                self._adjust_learning_rate(0.5)
                stagnation_counter = 0
            
            self.epoch_counter += 1
        
        print(f"\nâš ï¸ è¾¾åˆ°æœ€å¤§epochæ•° {max_total_epochs}ï¼Œåœæ­¢è®­ç»ƒ")
        print(f"æœ€ç»ˆæ·±åº¦: {self.model.get_current_depth()}å±‚/åˆ†æ”¯")
        print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_loss_history[-1]:.6f}")
        return False
    
    def _train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # å‰å‘ä¼ æ’­
            latents = batch['latents']
            target = batch['motion_params']
            target_frames = batch['target_frames']
            
            output = self.model(
                latents=latents,
                target_frames=target_frames,
                timestep=batch.get('timestep', None)
            )
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(output['motion_params'], target)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 10 == 0:
                print(f"  Batch {batch_idx}, Loss: {loss.item():.6f}")
        
        return total_loss / num_batches
    
    def _validate(self):
        """éªŒè¯"""
        if self.val_loader is None:
            return 0.0
        
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                latents = batch['latents']
                target = batch['motion_params']
                target_frames = batch['target_frames']
                
                output = self.model(
                    latents=latents,
                    target_frames=target_frames,
                    timestep=batch.get('timestep', None)
                )
                
                loss = self.criterion(output['motion_params'], target)
                total_loss += loss.item()
                num_batches += 1
        
        self.model.train()
        return total_loss / num_batches
    
    def _adjust_learning_rate(self, factor: float):
        """è°ƒæ•´å­¦ä¹ ç‡"""
        for param_group in self.optimizer.param_groups:
            param_group['lr'] *= factor
        print(f"å­¦ä¹ ç‡è°ƒæ•´ä¸º: {self.optimizer.param_groups[0]['lr']:.2e}")
    
    def save_checkpoint(self, path: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch_counter,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_loss_history': self.train_loss_history,
            'val_loss_history': self.val_loss_history,
            'depth_history': self.model.get_depth_history(),
            'current_depth': self.model.get_current_depth(),
        }
        torch.save(checkpoint, path)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {path}")
    
    def load_checkpoint(self, path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path, map_location='cpu')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epoch_counter = checkpoint['epoch']
        self.train_loss_history = checkpoint['train_loss_history']
        self.val_loss_history = checkpoint['val_loss_history']
        print(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: epoch {self.epoch_counter}, æ·±åº¦ {checkpoint['current_depth']}")


def demo_elastic_motion_decoder():
    """æ¼”ç¤ºå¼¹æ€§è¿åŠ¨è§£ç å™¨"""

    # åˆ›å»ºé…ç½®
    config = create_elastic_motion_decoder_config(
        dropout_rate=0.1,
        use_weight_decay=True,
        use_layer_norm=False,
        use_stochastic_depth=True,
        stochastic_depth_rate=0.1,
        max_res_layers=32,  # æœ€å¤§32å±‚
        initial_res_layers=2,  # åˆå§‹2å±‚
        use_elastic_depth=True
    )

    # åˆ›å»ºæ¨¡å‹
    motion_decoder = ElasticMotionDecoder.from_config(config)

    # æµ‹è¯•è¾“å…¥
    batch_size = 1
    latent_channels = 128
    T_compressed = 16
    n_persons = 2

    latents = torch.randn(batch_size, latent_channels, T_compressed, 1, n_persons)

    # ç›®æ ‡å¸§æ•°
    target_frames = 121

    # åˆå§‹å‰å‘ä¼ æ’­
    motion_output = motion_decoder(
        latents=latents,
        target_frames=target_frames,
        timestep=torch.tensor([0.5, 0.5])
    )

    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    print(f"è¾“å‡ºå½¢çŠ¶: {motion_output['motion_params'].shape}")
    print(f"åº”è¯¥ä¸º: [1, 69, 121, 1, 2]")
    print(f"å½“å‰æ·±åº¦: {motion_output['current_depth']}å±‚/åˆ†æ”¯")
    
    # è·å–ä¸‰ä¸ªåˆ†æ”¯çš„è¾“å‡º
    trans_out = motion_output['trans_output']
    root_out = motion_output['root_output']
    pose_out = motion_output['pose_output']
    
    print(f"\nä¸‰ä¸ªåˆ†æ”¯çš„è¾“å‡ºç»Ÿè®¡:")
    print(f"transå½¢çŠ¶: {trans_out.shape}, èŒƒå›´: [{trans_out.min():.3f}, {trans_out.max():.3f}]")
    print(f"rootå½¢çŠ¶: {root_out.shape}, èŒƒå›´: [{root_out.min():.3f}, {root_out.max():.3f}]")
    print(f"poseå½¢çŠ¶: {pose_out.shape}, èŒƒå›´: [{pose_out.min():.3f}, {pose_out.max():.3f}]")
    
    # æµ‹è¯•å¢åŠ æ·±åº¦
    print(f"\n=== æµ‹è¯•å¼¹æ€§å¢åŠ æ·±åº¦ ===")
    print(f"å¢åŠ æ·±åº¦å‰: {motion_decoder.get_current_depth()}å±‚/åˆ†æ”¯")
    
    # æ¨¡æ‹Ÿä¸€äº›è®­ç»ƒæŸå¤±
    for i in range(5):
        motion_decoder.record_loss(0.5 - i * 0.05)
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¢åŠ æ·±åº¦
    should_increase = motion_decoder.should_increase_depth(patience=3, min_improvement=0.01)
    print(f"åº”è¯¥å¢åŠ æ·±åº¦: {should_increase}")
    
    # æ‰‹åŠ¨å¢åŠ æ·±åº¦
    if motion_decoder.add_depth(num_layers=1):
        print(f"å¢åŠ æ·±åº¦å: {motion_decoder.get_current_depth()}å±‚/åˆ†æ”¯")
        
        # å†æ¬¡å‰å‘ä¼ æ’­
        motion_output2 = motion_decoder(
            latents=latents,
            target_frames=target_frames,
            timestep=torch.tensor([0.5, 0.5])
        )
        print(f"å¢åŠ æ·±åº¦åè¾“å‡ºå½¢çŠ¶: {motion_output2['motion_params'].shape}")
    
    # æµ‹è¯•è®¾ç½®ç›®æ ‡æ·±åº¦
    print(f"\n=== æµ‹è¯•è®¾ç½®ç›®æ ‡æ·±åº¦ ===")
    target_depth = 4
    if motion_decoder.set_depth(target_depth):
        print(f"æˆåŠŸè®¾ç½®æ·±åº¦åˆ°: {motion_decoder.get_current_depth()}å±‚/åˆ†æ”¯")
    else:
        print(f"è®¾ç½®æ·±åº¦å¤±è´¥")
    
    print(f"\næœ€å¤§å…è®¸æ·±åº¦: {motion_decoder.get_max_depth()}å±‚/åˆ†æ”¯")
    print(f"æ˜¯å¦è¾¾åˆ°æœ€å¤§æ·±åº¦: {motion_decoder.is_at_max_depth()}")

    return motion_decoder


if __name__ == "__main__":
    demo_elastic_motion_decoder()